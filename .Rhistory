View(merged_df)
# Assuming your data frame is named df
merged_df <- merged_df[197:386, ]
# Assuming 'merged_df' is your merged data frame
cleaned_df <- na.omit(merged_df)
# View the cleaned data frame
View(cleaned_df)
# Assuming you've already loaded dplyr and lubridate libraries
# If not, uncomment the next two lines
library(dplyr)
# library(lubridate)
final <- cleaned_df %>%
mutate(
Output_growth = log(`real gdp` / population) * 100,
Consumption_growth = log((PCE / `gdp deflator`) / population) * 100,
Investment_growth = log((fpi / `gdp deflator`) / population) * 100,
Wage_growth = log(compensation / `gdp deflator`) * 100,
Hours = log((`weekly_production_hours` * (employment / 100) / population)) * 100,
Inflation = log(`gdp deflator` / lag(`gdp deflator`, 1)) * 100,
Ffr = FEDFUNDS / 4
)
# Real-time running average calculation
real_time_running_average <- function(data, i, window_size) {
if (i < window_size) {
# If less data than window size, calculate mean of available data
return(mean(data[1:i]))
} else {
# Calculate running average of the window size ending at current index i
return(mean(data[(i-window_size+1):i]))
}
}
final=final[2:64,]
# Initialize the dataframe with the first 4rhows set to 1
my_data <- data.frame(
time = 1:63,
K_hat=rep(350,63),
r_k_star=rep(-500,63),
Q_hat = rep(14,63),
pi_bar = rep(3,63),
epsilon_C = rep(1, 63),
epsilon_Q = rep(1, 63),
epsilon_I = rep(1,63),
epsilon_W = rep(1, 63),
epsilon_R = rep(1, 63),
epsilon_Pi=rep(1,63),
epsilon_L=rep(1,63),
epsilon_G=rep(1,63),
epsilon_pitwo=rep(1,63),
expected_K_hat_next=rep(350,63),
expected_Q_hat_next = rep(14, 63),
# Lagged values for the firstrhow would be NA
Q_hat_lag1 = rep(14, 63),
K_hat_lag1 = rep(350,63)
)
my_data$C_hat =final$Consumption_growth
my_data$C_hat_lag1=final$Consumption_growth
my_data$expected_C_hat_next=final$Consumption_growth
my_data$I_hat_lag1 = final$Investment_growth
my_data$expected_I_hat_next=final$Investment_growth
my_data$I_hat = final$Investment_growth
my_data$I_hat_lag1 = final$Investment_growth
Y_hat=final$Output_growth
Y_hat_lag1=final$Output_growth
my_data$expected_Y_hat_next = final$Output_growth
my_data$Pi_hat=final$Inflation
my_data$Pi_hat_lag1=final$Inflation
my_data$expected_Pi_hat_next=final$Inflation
my_data$R_hat=final$FEDFUNDS
my_data$R_hat_lag1=final$FEDFUNDS
my_data$expected_R_hat_next=final$FEDFUNDS
my_data$L_hat=final$Hours
my_data$L_hat_lag1=final$Hours
my_data$expected_L_hat_next=final$Hours
my_data$W_hat=final$Wage_growth
my_data$W_hat_lag1=final$Wage_growth
my_data$expected_W_hat_next=final$Wage_growth
View(final)
# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 2:63) {
# Update lagged values
my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
# Generate new shock value for epsilon_c for the currentrhow
eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
# Simulate a persistence parameter from the Beta distribution
rho_c<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
# Generate new shock value for epsilon_q for the current row
epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
my_data$epsilon_Q[i] <- epsilon_Q
eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
rho_q<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
my_data$epsilon_W[i]=epsilon_W
epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
my_data$epsilon_Pi[i]=epsilon_p
rho_pi<- rbeta(1, alpha_beta, beta_beta)
eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
rho_g<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_G[i] <- rho_g * my_data$epsilon_G[i-1] + eta_g
epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
my_data$epsilon_W[i]=epsilon_W
epsilon_R<- rnorm(1, mean = 0, sd = sigma_er)
my_data$epsilon_R[i]=epsilon_R
# Output gap equation
my_data$Q_hat[i] = (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i]) +
((1-tao) / (1 - tao + my_data$r_k_star[i])) * my_data$expected_Q_hat_next[i] +
(my_data$r_k_star[i] / (1 - tao + my_data$r_k_star[i])) * my_data$expected_R_hat_next[i] + my_data$epsilon_Q[i]
# Update pi_bar using the running average of Pi_hat
my_data$pi_bar[i] <- real_time_running_average(my_data$Pi_hat, i, window_size = 5)  # window size is an example, adjust as needed
my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
tao*my_data$epsilon_I[i]
my_data$r_k_star[i]=-(my_data$K_hat[i]- my_data$L_hat[i])+my_data$W_hat[i]
}
# Calculate 'T' as the number of observations
T <- nrow(final)  # This should be a single integer, not a vector
# Combine vectors into a new dataframe with 'T' as a separate element
data_list <- list(
T = T,  # 'T' is a single integer, not a vector
epsilon_C = my_data$epsilon_C,
epsilon_Q = my_data$epsilon_Q,
epsilon_I = my_data$epsilon_I,
epsilon_PI = my_data$epsilon_Pi,
epsilon_G = my_data$epsilon_G,
epsilon_R = my_data$epsilon_R,
r_k_star=my_data$r_k_star,
epsilon_W = my_data$epsilon_W,
output_growth_vector = final$Output_growth,
consumption_growth_vector = final$Consumption_growth,
investment_growth_vector = final$Investment_growth,
wage_growth_vector = final$Wage_growth,
hours_vector = final$Hours,
inflation_vector = final$Inflation,
ffr_vector = final$Ffr,
kapital = my_data$K_hat,
q = my_data$Q_hat,
pi_bar=my_data$pi_bar,
epsilon_pitwo= my_data$epsilon_pitwo,
epsilon_L= my_data$epsilon_L
)
# Ensure the list is correctly structured for Stan
if(!all(sapply(data_list[-1], length) == T)) {  # Exclude 'T' from the length check
stop("The lengths of the vectors do not match the value of T.")
}
fit <- sampling(model, data =data_list , chains = 4, iter = 6000, warmup = 1000)
library(rstan)
fit <- sampling(model, data =data_list , chains = 4, iter = 6000, warmup = 1000)
fit <- sampling(model, data =data_list , chains = 4, iter = 6000, warmup = 1000)
# Assuming 'fit' is your Stan model object
summary_stats <- summary(fit)$summary
# Convert the matrix to a data frame where rows are parameters and columns are summary statistics
df_summary_stats <- as.data.frame(summary_stats)
# Set the row names as a column if needed
df_summary_stats$parameter <- rownames(df_summary_stats)
# Now df_summary_stats is a data frame with each parameter as a row and statistics as columns
print(df_summary_stats)
View(df_summary_stats)
Beta=.025
tao=0.025
alpha =0.3
psi=1/.169
gamma_p=.469
gamma_w=.763
lambda_w=.5
zeta_ps=.908
zeta_ws=0.737
sigma_L=2.4
sigma_c=1.353
h=0.573
phi=1.408
phi_uppercase=1/6.771
rbar_k=(1/Beta)-1+tao
Ky=8.8
sigma_eta_C<- 0.336
Invy=22
Cy=0.6
Ky=Invy/tao
gy=1-Cy-Invy
r_pi_g=0.14
r_y=0.099
Ry_delta=0.159
rho=0.961
r_pi=1.684
rho_el=0.889
rho_ea=0.823
rho_eb=0.855
rho_G=0.949
rho_pi=0.924
rh_i=0.927
rho_er=0
r_k_star <- (1 / Beta) - 1 + tao
rho_lamdaw=0
rho_q=0
rho_lamda_p=0
rho_L <-   0.9
sigma_e_L=3.52
sigma_e_a=.598
sigma_e_b=.336
sigma_G=.325
sigma_pi_bar=.017
sigma_er=0.081
sigma_ei=0.085
sigma_lamada_p=.16
sigma_lamda_w=.289
sigma_e_q=.604
sigma_e_i <- 3.52
sigma_e_c<- 0.336
alpha_k <- 0.3
sigma_e_p <- 0.16 # Standard deviation for price markup shocks
sigma_e_w <- 0.289 # Standard deviation for wage markup shocks
sigma_e_q <- 0.604 # Standard deviation for equity price markup shocks
# Define parameters for the distributions
mean_inverse_gamma <- 0.25
mean_inverse_gamma_pi_star <- 0.05
degrees_of_freedom <- 2
mean_beta <- 0.85
sd_beta <- 0.1
# For the Beta distribution parameters, convert mean and standard deviation to alpha and beta parameters
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)
# Define the Inverse-Gamma distribution for the standard errors of the innovations
sigma_e <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma * degrees_of_freedom))
sigma_e_pi_star <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma_pi_star * degrees_of_freedom))
# Convert from gamma to inverse gamma since R does not have a direct inverse gamma function
sigma_e <- 1 / sigma_e
sigma_e_pi_star <- 1 / sigma_e_pi_star
# Define the Beta distribution for the persistence parameters of the AR(1) processes
rho_ar1 <- rbeta(1, alpha_beta, beta_beta)
calculate_ema_4th_order <- function(data, n=4) {
# Ensure data is in correct format (numeric vector)
if (!is.numeric(data)) {
stop("Data must be a numeric vector.")
}
alpha <- 2 / (n + 1)
ema <- numeric(length(data))
ema[1] <- data[1]  # Initialize the first EMA value to the first data point
for (t in 2:length(data)) {
ema[t] <- alpha * data[t] + (1 - alpha) * ema[t-1]
}
# To calculate expected next value, we simply apply the formula one more time assuming last value repeats
expected_next_ema <- alpha * data[length(data)] + (1 - alpha) * ema[length(ema)]
return(list(ema = ema, expected_next = expected_next_ema))
}
# Now, apply the calculate_ema_4th_order function to this lagged series
# Assuming you have initialized your dataframe and it's being updated elsewhere in your simulation
calculate_ema_4th_order_for_df <- function(df, column_name, n=4) {
# Extract the column data based on column_name
data <- df[[column_name]]
# Filter out NA values that arise due to lagging
data <- na.omit(data)
# Apply the EMA calculation
ema_result <- calculate_ema_4th_order(data, n)
# Return the result
return(ema_result)
}
# Define the parameters for the Beta distribution based on the given mean and standard deviation
mean_beta <- 0.85
sd_beta <- 0.1
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)
library(readxl)
# Attempt to read the file without the leading directory path
averageweeklyproductionhours <- read_excel("averageweeklyproductionhours.xls")
employment=read_excel("Employment .xls")
gdpdeflator =read_excel("gdpdeflator.xlsx")
population =read_excel("population .xlsx")
investment =read_excel("fixedprivateinvestment .xls")
fedfunds=read_excel("FEDFUNDS.xls")
personalconsumption=read_excel("personalconsumption.xlsx")
realGDP=read_excel("realgdp.xlsx")
compensation =read_excel("compensation .xlsx")
# List of your data frames
df_list <- list(averageweeklyproductionhours, employment, gdpdeflator, population, investment, fedfunds, personalconsumption, realGDP,compensation)
# Function to convert the first column to Date and the second to numeric
convert_columns <- function(df) {
df[[1]] <- as.Date(df[[1]], format = "%Y-%m-%d") # Adjust format as necessary
df[[2]] <- as.numeric(df[[2]])
return(df)
}
# Apply the function to each data frame in the list
converted_df_list <- lapply(df_list, convert_columns)
# If you want to replace the original data frames in your environment with the converted ones:
names(converted_df_list) <- c("averageweeklyproductionhours", "employment", "gdpdeflator", "population", "investment", "fedfunds", "personalconsumption", "realGDP","compensation")
list2env(converted_df_list, envir = .GlobalEnv)
# Assuming the converted data frames are stored in 'converted_df_list' and all have a common date column named 'Date'
# If the date column has different names across data frames, you'll need to rename them to be consistent first
# Example of renaming the first column to 'Date' for all data frames if needed
converted_df_list <- lapply(converted_df_list, function(df) {
names(df)[1] <- 'Date'
return(df)
})
# Merging all data frames on the 'Date' column
merged_df <- Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE), converted_df_list)
# The 'all = TRUE' argument ensures that all rows from each data frame are kept in the merged data frame,
# even if there are missing matches in the 'Date' column from any of the data frames
# View the merged data frame
View(merged_df)
# Assuming your data frame is named df
merged_df <- merged_df[387:722, ]
# Assuming 'merged_df' is your merged data frame
cleaned_df <- na.omit(merged_df)
# View the cleaned data frame
View(cleaned_df)
# Assuming you've already loaded dplyr and lubridate libraries
# If not, uncomment the next two lines
library(dplyr)
# library(lubridate)
final <- cleaned_df %>%
mutate(
Output_growth = log(`real gdp` / population) * 100,
Consumption_growth = log((PCE / `gdp deflator`) / population) * 100,
Investment_growth = log((fpi / `gdp deflator`) / population) * 100,
Wage_growth = log(compensation / `gdp deflator`) * 100,
Hours = log((`weekly_production_hours` * (employment / 100) / population)) * 100,
Inflation = log(`gdp deflator` / lag(`gdp deflator`, 1)) * 100,
Ffr = FEDFUNDS / 4
)
# Real-time running average calculation
real_time_running_average <- function(data, i, window_size) {
if (i < window_size) {
# If less data than window size, calculate mean of available data
return(mean(data[1:i]))
} else {
# Calculate running average of the window size ending at current index i
return(mean(data[(i-window_size+1):i]))
}
}
final=final[2:112,]
# Initialize the dataframe with the first 4rhows set to 1
my_data <- data.frame(
time = 1:111,
K_hat=rep(350,111),
r_k_star=rep(-500,111),
Q_hat = rep(14,111),
pi_bar = rep(3,111),
epsilon_C = rep(1, 111),
epsilon_Q = rep(1, 111),
epsilon_I = rep(1,111),
epsilon_W = rep(1, 111),
epsilon_R = rep(1, 111),
epsilon_Pi=rep(1,111),
epsilon_L=rep(1,111),
epsilon_G=rep(1,111),
epsilon_pitwo=rep(1,111),
expected_K_hat_next=rep(350,111),
expected_Q_hat_next = rep(14, 111),
# Lagged values for the firstrhow would be NA
Q_hat_lag1 = rep(14, 111),
K_hat_lag1 = rep(350,111)
)
my_data$C_hat =final$Consumption_growth
my_data$C_hat_lag1=final$Consumption_growth
my_data$expected_C_hat_next=final$Consumption_growth
my_data$I_hat_lag1 = final$Investment_growth
my_data$expected_I_hat_next=final$Investment_growth
my_data$I_hat = final$Investment_growth
my_data$I_hat_lag1 = final$Investment_growth
Y_hat=final$Output_growth
Y_hat_lag1=final$Output_growth
my_data$expected_Y_hat_next = final$Output_growth
my_data$Pi_hat=final$Inflation
my_data$Pi_hat_lag1=final$Inflation
my_data$expected_Pi_hat_next=final$Inflation
my_data$R_hat=final$FEDFUNDS
my_data$R_hat_lag1=final$FEDFUNDS
my_data$expected_R_hat_next=final$FEDFUNDS
my_data$L_hat=final$Hours
my_data$L_hat_lag1=final$Hours
my_data$expected_L_hat_next=final$Hours
my_data$W_hat=final$Wage_growth
my_data$W_hat_lag1=final$Wage_growth
my_data$expected_W_hat_next=final$Wage_growth
# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 2:111) {
# Update lagged values
my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
# Generate new shock value for epsilon_c for the currentrhow
eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
# Simulate a persistence parameter from the Beta distribution
rho_c<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
# Generate new shock value for epsilon_q for the current row
epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
my_data$epsilon_Q[i] <- epsilon_Q
eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
rho_q<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
my_data$epsilon_W[i]=epsilon_W
epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
my_data$epsilon_Pi[i]=epsilon_p
rho_pi<- rbeta(1, alpha_beta, beta_beta)
eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
rho_g<- rbeta(1, alpha_beta, beta_beta)
my_data$epsilon_G[i] <- rho_g * my_data$epsilon_G[i-1] + eta_g
epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
my_data$epsilon_W[i]=epsilon_W
epsilon_R<- rnorm(1, mean = 0, sd = sigma_er)
my_data$epsilon_R[i]=epsilon_R
# Output gap equation
my_data$Q_hat[i] = (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i]) +
((1-tao) / (1 - tao + my_data$r_k_star[i])) * my_data$expected_Q_hat_next[i] +
(my_data$r_k_star[i] / (1 - tao + my_data$r_k_star[i])) * my_data$expected_R_hat_next[i] + my_data$epsilon_Q[i]
# Update pi_bar using the running average of Pi_hat
my_data$pi_bar[i] <- real_time_running_average(my_data$Pi_hat, i, window_size = 5)  # window size is an example, adjust as needed
my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
tao*my_data$epsilon_I[i]
my_data$r_k_star[i]=-(my_data$K_hat[i]- my_data$L_hat[i])+my_data$W_hat[i]
}
library(rstan)
# Calculate 'T' as the number of observations
T <- nrow(final)  # This should be a single integer, not a vector
# Combine vectors into a new dataframe with 'T' as a separate element
data_list <- list(
T = T,  # 'T' is a single integer, not a vector
epsilon_C = my_data$epsilon_C,
epsilon_Q = my_data$epsilon_Q,
epsilon_I = my_data$epsilon_I,
epsilon_PI = my_data$epsilon_Pi,
epsilon_G = my_data$epsilon_G,
epsilon_R = my_data$epsilon_R,
r_k_star=my_data$r_k_star,
epsilon_W = my_data$epsilon_W,
output_growth_vector = final$Output_growth,
consumption_growth_vector = final$Consumption_growth,
investment_growth_vector = final$Investment_growth,
wage_growth_vector = final$Wage_growth,
hours_vector = final$Hours,
inflation_vector = final$Inflation,
ffr_vector = final$Ffr,
kapital = my_data$K_hat,
q = my_data$Q_hat,
pi_bar=my_data$pi_bar,
epsilon_pitwo= my_data$epsilon_pitwo,
epsilon_L= my_data$epsilon_L
)
# Ensure the list is correctly structured for Stan
if(!all(sapply(data_list[-1], length) == T)) {  # Exclude 'T' from the length check
stop("The lengths of the vectors do not match the value of T.")
}
library(V8)
fit <- sampling(model, data =data_list , chains = 4, iter = 6000, warmup = 1000)
library(bayesplot)
setwd("C:/Users/gealy/OneDrive/Documents/New folder (2)")
getwd()
# Assuming 'fit' is your Stan model object
summary_stats <- summary(fit)$summary
# Convert the matrix to a data frame where rows are parameters and columns are summary statistics
df_summary_stats <- as.data.frame(summary_stats)
# Set the row names as a column if needed
df_summary_stats$parameter <- rownames(df_summary_stats)
# Now df_summary_stats is a data frame with each parameter as a row and statistics as columns
print(df_summary_stats)
options(scipen = 999)
# Ensure ggplot2 is loaded
library(ggplot2)
setwd("C:/Users/gealy/OneDrive/Documents/New folder (2)")
getwd()
# Ensure params are extracted with structure maintained
params <- extract(fit, permuted=FALSE)
param_names <- dimnames(params)[[3]]  # Extract parameter names
color_scheme_set("brightblue")
# Loop through each parameter and generate plots
for (param_name in param_names) {
if (param_name != "lp__") {  # Skip the log probability parameter
param_data <- as.array(params[,, param_name])
if (is.array(param_data)) {
# Create a data frame for ggplot
param_df <- data.frame(
Iteration = rep(1:nrow(param_data), ncol(param_data)),
Value = as.vector(t(param_data)),
Chain = rep(1:ncol(param_data), each = nrow(param_data)),
Parameter = param_name  # Include parameter name for clarity
)
# Generate the plot
p <- ggplot(param_df, aes(x = Iteration, y = Value, color = as.factor(Chain))) +
geom_line() +
labs(title = paste("Trace Plot for", param_name), x = "Iteration", y = param_name) +
theme_minimal()
# Print the plot to the console or save to file
print(p)
# Optionally, save the plot to a file
ggsave(paste0("Trace_", param_name, ".png"), plot = p, width = 10, height = 8)
# Clear the plot if running in a script to save memory
dev.off()  # This resets the plotting device
} else {
cat(param_name, "data could not be converted to an array correctly.\n")
}
}
}
View(df_summary_stats)
View(final)

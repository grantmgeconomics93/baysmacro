---
title: "baysmacro"
output: html_document
date: "2024-03-01"
---

```{r}
Beta=.025
tao=0.025
alpha =0.3
psi=1/.169
gamma_p=.469
gamma_w=.763
lambda_w=.5
zeta_ps=.908
zeta_ws=0.737
sigma_L=2.4
sigma_c=1.353
h=0.573
phi=1.408
phi_uppercase=1/6.771
rbar_k=(1/Beta)-1+tao
Ky=8.8
sigma_eta_C<- 0.336
Invy=22
Cy=0.6
ky=Invy/tao
gy=1-Cy-Invy
r_pi_g=0.14
r_y=0.099
Ry_delta=0.159
rho=0.961
r_pi=1.684
rho_el=0.889
rho_ea=0.823
rho_eb=0.855
rho_G=0.949
rho_pi=0.924
 rh_i=0.927
rho_er=0
r_k_star <- (1 / Beta) - 1 + tao
rho_lamdaw=0
rho_q=0
rho_lamda_p=0
rho_L <-   0.9
sigma_e_L=3.52
sigma_e_a=.598
sigma_e_b=.336
sigma_G=.325
sigma_pi_bar=.017
sigma_er=0.081
sigma_ei=0.085
sigma_lamada_p=.16
sigma_lamda_w=.289
sigma_e_q=.604
sigma_e_i <- 3.52
sigma_e_c<- 0.336
alpha_k <- 0.3 
sigma_e_p <- 0.16 # Standard deviation for price markup shocks
sigma_e_w <- 0.289 # Standard deviation for wage markup shocks
sigma_e_q <- 0.604 # Standard deviation for equity price markup shocks


```
```{r}
# Define parameters for the distributions
mean_inverse_gamma <- 0.25
mean_inverse_gamma_pi_star <- 0.05
degrees_of_freedom <- 2
mean_beta <- 0.85
sd_beta <- 0.1

# For the Beta distribution parameters, convert mean and standard deviation to alpha and beta parameters
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)

# Define the Inverse-Gamma distribution for the standard errors of the innovations
sigma_e <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma * degrees_of_freedom))
sigma_e_pi_star <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma_pi_star * degrees_of_freedom))

# Convert from gamma to inverse gamma since R does not have a direct inverse gamma function
sigma_e <- 1 / sigma_e
sigma_e_pi_star <- 1 / sigma_e_pi_star

# Define the Beta distribution for the persistence parameters of the AR(1) processes
rho_ar1 <- rbeta(1, alpha_beta, beta_beta)
```

```{r}
calculate_ema_4th_order <- function(data, n=4) {
  # Ensure data is in correct format (numeric vector)
  if (!is.numeric(data)) {
    stop("Data must be a numeric vector.")
  }

  alpha <- 2 / (n + 1)
  ema <- numeric(length(data))
  ema[1] <- data[1]  # Initialize the first EMA value to the first data point
  
  for (t in 2:length(data)) {
    ema[t] <- alpha * data[t] + (1 - alpha) * ema[t-1]
  }
  
  # To calculate expected next value, we simply apply the formula one more time assuming last value repeats
  expected_next_ema <- alpha * data[length(data)] + (1 - alpha) * ema[length(ema)]
  return(list(ema = ema, expected_next = expected_next_ema))
}



# Now, apply the calculate_ema_4th_order function to this lagged series
# Assuming you have initialized your dataframe and it's being updated elsewhere in your simulation

calculate_ema_4th_order_for_df <- function(df, column_name, n=4) {
  # Extract the column data based on column_name
  data <- df[[column_name]]
  
  # Filter out NA values that arise due to lagging
  data <- na.omit(data)
  
  # Apply the EMA calculation
  ema_result <- calculate_ema_4th_order(data, n)
  
  # Return the result
  return(ema_result)
}


```


```{r}
# Define the parameters for the Beta distribution based on the given mean and standard deviation
mean_beta <- 0.85
sd_beta <- 0.1
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)


```


```{r}
library(readxl)
# Attempt to read the file without the leading directory path
averageweeklyproductionhours <- read_excel("averageweeklyproductionhours.xls")
employment=read_excel("Employment .xls")
gdpdeflator =read_excel("gdpdeflator.xlsx")
population =read_excel("population .xlsx")
investment =read_excel("fixedprivateinvestment .xls")
fedfunds=read_excel("FEDFUNDS.xls")
personalconsumption=read_excel("personalconsumption.xlsx")
realGDP=read_excel("realgdp.xlsx")
compensation =read_excel("compensation .xlsx")


```
```{r}
# List of your data frames
df_list <- list(averageweeklyproductionhours, employment, gdpdeflator, population, investment, fedfunds, personalconsumption, realGDP,compensation)

# Function to convert the first column to Date and the second to numeric
convert_columns <- function(df) {
  df[[1]] <- as.Date(df[[1]], format = "%Y-%m-%d") # Adjust format as necessary
  df[[2]] <- as.numeric(df[[2]])
  return(df)
}

# Apply the function to each data frame in the list
converted_df_list <- lapply(df_list, convert_columns)

# If you want to replace the original data frames in your environment with the converted ones:
names(converted_df_list) <- c("averageweeklyproductionhours", "employment", "gdpdeflator", "population", "investment", "fedfunds", "personalconsumption", "realGDP","compensation")
list2env(converted_df_list, envir = .GlobalEnv)

```
```{r}
# Assuming the converted data frames are stored in 'converted_df_list' and all have a common date column named 'Date'
# If the date column has different names across data frames, you'll need to rename them to be consistent first

# Example of renaming the first column to 'Date' for all data frames if needed
converted_df_list <- lapply(converted_df_list, function(df) {
  names(df)[1] <- 'Date'
  return(df)
})

# Merging all data frames on the 'Date' column
merged_df <- Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE), converted_df_list)

# The 'all = TRUE' argument ensures that all rows from each data frame are kept in the merged data frame, 
# even if there are missing matches in the 'Date' column from any of the data frames

# View the merged data frame
View(merged_df)

```
```{r}

# Assuming your data frame is named df
merged_df <- merged_df[265:629, ]


```

```{r}
# Assuming 'merged_df' is your merged data frame
cleaned_df <- na.omit(merged_df)

# View the cleaned data frame
View(cleaned_df)

```
```{r}
# Assuming you've already loaded dplyr and lubridate libraries
# If not, uncomment the next two lines
 library(dplyr)
# library(lubridate)

final <- cleaned_df %>%
  mutate(
    Output_growth = log(`real gdp` / population) * 100,
    Consumption_growth = log((PCE / `gdp deflator`) / population) * 100,
    Investment_growth = log((fpi / `gdp deflator`) / population) * 100,
    Wage_growth = log(compensation / `gdp deflator`) * 100,
    Hours = log((`weekly_production_hours` * (employment / 100) / population)) * 100,
    Inflation = log(`gdp deflator` / lag(`gdp deflator`, 1)) * 100,
    Ffr = FEDFUNDS / 4
  )

```
```{r}
# Assuming your data frame is named df
final <- final[2:121, ]
```


```{r}
# Initialize the dataframe with the first 4rhows set to 1
my_data <- data.frame(
    time = 1:120,
   
    
    K_hat=rep(350,120),
    r_k_star=rep(-500,120),
    
   
    Q_hat = rep(14,120),
    
    epsilon_C = rep(1, 120),
    epsilon_Q = rep(1, 120),
     epsilon_I = rep(1,120),
    epsilon_W = rep(1, 120),
    epsilon_R = rep(1, 120),
    epsilon_Pi=rep(1,120),
    epsilon_L=rep(1,120),
    epsilon_G=rep(1,120),
    
     epsilon_pitwo=rep(1,120),
    expected_K_hat_next=rep(350,120),
    

    expected_Q_hat_next = rep(14, 120),
   
  

   # Lagged values for the firstrhow would be NA
    Q_hat_lag1 = rep(14, 120),
    
    K_hat_lag1 = rep(350,120)
)
 my_data$C_hat =final$Consumption_growth
 my_data$C_hat_lag1=final$Consumption_growth
 my_data$expected_C_hat_next=final$Consumption_growth
  my_data$I_hat_lag1 = final$Investment_growth
    my_data$expected_I_hat_next=final$Investment_growth
   my_data$I_hat = final$Investment_growth
   my_data$I_hat_lag1 = final$Investment_growth
    Y_hat=final$Output_growth
    Y_hat_lag1=final$Output_growth
     my_data$expected_Y_hat_next = final$Output_growth
     my_data$Pi_hat=final$Inflation
     my_data$Pi_hat_lag1=final$Inflation
     my_data$expected_Pi_hat_next=final$Inflation
     my_data$R_hat=final$FEDFUNDS
     my_data$R_hat_lag1=final$FEDFUNDS
     my_data$expected_R_hat_next=final$FEDFUNDS
     my_data$L_hat=final$Hours
     my_data$L_hat_lag1=final$Hours
     my_data$expected_L_hat_next=final$Hours
     my_data$W_hat=final$Wage_growth
     my_data$W_hat_lag1=final$Wage_growth
     my_data$expected_W_hat_next=final$Wage_growth
     
```
```{r}

# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 2:120) {
    # Update lagged values
    my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
    my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
    my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
   
    my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
    my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
    my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
    
    my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
    my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
     my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
    
    
    
     # Generate new shock value for epsilon_c for the currentrhow
    eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
    
# Simulate a persistence parameter from the Beta distribution
    rho_c<- rbeta(1, alpha_beta, beta_beta)
     my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
       # Generate new shock value for epsilon_q for the current row
     epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
       my_data$epsilon_Q[i] <- epsilon_Q
       eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
       rho_q<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
      
       epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
      epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
      my_data$epsilon_Pi[i]=epsilon_p
      
        rho_pi<- rbeta(1, alpha_beta, beta_beta)
        eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
       my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
        eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
        rho_g<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_G[i] <- rho_g * my_data$epsilon_G[i-1] + eta_g
       epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
       epsilon_R<- rnorm(1, mean = 0, sd = sigma_er)
       my_data$epsilon_R[i]=epsilon_R
        
  # Output gap equation
my_data$Q_hat[i] = (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i]) +
       ((1-tao) / (1 - tao + my_data$r_k_star[i])) * my_data$expected_Q_hat_next[i] +
        (my_data$r_k_star[i] / (1 - tao + my_data$r_k_star[i])) * my_data$expected_R_hat_next[i] + my_data$epsilon_Q[i]

 
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
  my_data$r_k_star[i]=-(my_data$K_hat[i]- my_data$L_hat[i])+my_data$W_hat[i]
    
    }

  
```



```{r}
library(rstan)
```


```{r}
# Calculate 'T' as the number of observations
T <- nrow(final)  # This should be a single integer, not a vector

# Combine vectors into a new dataframe with 'T' as a separate element
data_list <- list(
  T = T,  # 'T' is a single integer, not a vector
  epsilon_C = my_data$epsilon_C,
  epsilon_Q = my_data$epsilon_Q,
  epsilon_I = my_data$epsilon_I,
  epsilon_PI = my_data$epsilon_Pi,
  epsilon_G = my_data$epsilon_G,
  epsilon_R = my_data$epsilon_R,
  r_k_star=my_data$r_k_star,
  epsilon_W = my_data$W_hat,
  output_growth_vector = final$Output_growth,
  consumption_growth_vector = final$Consumption_growth,
  investment_growth_vector = final$Investment_growth,
  wage_growth_vector = final$Wage_growth,
  hours_vector = final$Hours,
  inflation_vector = final$Inflation,
  ffr_vector = final$Ffr,
  kapital = my_data$K_hat,
  q = my_data$Q_hat
)

# Ensure the list is correctly structured for Stan
if(!all(sapply(data_list[-1], length) == T)) {  # Exclude 'T' from the length check
  stop("The lengths of the vectors do not match the value of T.")
}



```

```{r}
library(V8)
```

```{stan output.var="model"}
data {
  int<lower=1> T; // Number of time points
  vector[T] epsilon_C;
  vector[T] epsilon_Q;
  vector[T] epsilon_I;
  vector[T] epsilon_PI;
   vector[T] epsilon_G;
   vector[T] epsilon_R;
   vector[T] epsilon_W;
  vector[T] consumption_growth_vector ;    // Observed data
  vector[T] output_growth_vector;
  vector[T] investment_growth_vector;
  vector[T] wage_growth_vector;
  vector[T] hours_vector;
  vector[T] inflation_vector;
  vector[T] ffr_vector;
  vector[T] kapital;
  vector[T] q;
  vector[T]  r_k_star;
    // Standard deviation for eta_c
} 

parameters {
 
    real<lower=0.19, upper=0.75> Beta; // Given mean is 0.025, the range would be truncated because Beta cannot be negative
  real<lower=0.19, upper=0.75> tao;
  real<lower=0, upper=0.8> alpha; // Mean=0.3, so range is 0.3±0.5, lower bound truncated at 0
  real<lower=0, upper=1> psi; // Mean=1/.169, requires calculating the actual value and then ±0.5, truncating bounds as necessary
  real<lower=0, upper=0.969> gamma_pi; // Mean=0.469, so range is 0.469±0.5, upper bound truncated at 1
  real<lower=0, upper=1> gamma_w; // Mean=0.763, so range is 0.763±0.5, upper bound truncated at 1
    
  real<lower=0, upper=1> lambda_w;
  real<lower=0, upper=1> zeta_ps;
  real<lower=0, upper=1> zeta_ws;
  real<lower=1.9, upper=2.9> sigma_L;
  real<lower=0.853, upper=1.853> sigma_c;
  real<lower=0.073, upper=1> h;
  real<lower=0.908, upper=1.908> phi;
  real<lower=0, upper=1> phi_uppercase; // Mean=1/6.771, requires calculating the actual value and then ±0.5, truncating bounds as necessary
  real<lower=0, upper=1> rbar_k; // Depends on Beta, the range might need to be dynamically calculated
  real<lower=8.3, upper=9.3> Ky;
  real<lower=0, upper=0.836> sigma_eta_C;
  real<lower=21.5, upper=22.5> Invy;
  real<lower=0.1, upper=1.1> Cy; // Mean=0.6, so range is 0.6±0.5, upper bound truncated at 1
  real<lower=0, upper=1> ky; // Computed from Invy/tao, so range needs to be calculated based on those values
  real<lower=0, upper=1> gy; // Computed from 1-Cy-Invy, so range needs to be calculated based on those values
  real<lower=0, upper=0.64> r_pi;
  real<lower=0, upper=0.599> r_y;
  real<lower=0, upper=0.659> Ry_delta;
   real<lower=0, upper=1> rho;
  real<lower=0, upper=1> rho_el;
  real<lower=0, upper=1> rho_ea;
  real<lower=0, upper=1> rho_eb;
  real<lower=0, upper=1> rho_G;
  real<lower=0, upper=1> rho_pi;
  real<lower=0, upper=1> rh_i;
  
  
  real<lower=0> rho_er; // Since rho_er=0, it is set as the lower bound
  real<lower=0, upper=1> rho_lamdaw; // Since rho_lamdaw=0, it is set as the lower bound
  real<lower=0, upper=1> rho_q; // Since rho_q=0, it is set as the lower bound
  real<lower=0, upper=1> rho_lamda_p; // Since rho_lamda_p=0, it is set as the lower bound
  
  // r_k_star is computed from Beta and tao, and will need to have a dynamic range based on those values.
 
  // rho_L has a given value of 0.9, the range would be 0.4 to 1 since it can't exceed 1
  real<lower=0.4, upper=1> rho_L;
  // real<lower=0> sigma_e_L; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_a; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_b; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_G; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_pi_bar; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_er; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_ei; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_lamada_p; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_lamda_w; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_q; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_i; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_c; // Only lower bound specified, no upper bound
  //real<lower=0> alpha_inv_gamma;
  //real<lower=0> alpha_k; // Only lower bound specified, no upper bound

  //real<lower=0> sigma_e_p; // Only lower bound specified, no upper bound
  //real<lower=0> sigma_e_w; // Only lower bound specified, no upper bound

  
  // Continue with other parameters following the same pattern
}
transformed parameters  {
 
 // real<lower=(1/Beta)-1+tao-0.5, upper=(1/Beta)-1+tao+0.5> r_k_star;
}
model {
  
  // Priors for parameters based on the provided table
  phi ~ normal(4.00, 1.50);
  //sigma_c ~ normal(1.50, 0.37);
  h ~ beta(0.3, 14);  // Beta parameters calculated from mean and standard deviation
  zeta_ws ~ beta(0.3, 12);  // Mean=0.5, SD=0.10 implies alpha=beta
  //sigma_e_L ~ normal(2.00, 0.75);
  zeta_ps ~ beta(0.3, 12);  // Mean=0.5, SD=0.10 implies alpha=beta
  lambda_w ~ beta(0.3, 5.055);  // Mean=0.5, SD=0.15 implies alpha=beta
  psi ~ beta(0.3, 5.05);  // Mean=0.5, SD=0.15 implies alpha=beta
  phi_uppercase ~ normal(1.25, 0.12);
  r_pi ~ normal(1.50, 0.25);
  
  r_y ~ normal(0.12, 0.05);
  Ry_delta ~ normal(0.12, 0.05);
  
  rbar_k ~ normal(0.40, 0.10);
  alpha ~ normal(0.30, 0.05);
   // Inverse gamma priors for standard deviation parameters
 
  //sigma_e_L ~ inv_gamma(alpha_inv_gamma, 3.52);
  //sigma_e_a ~ inv_gamma(alpha_inv_gamma, 0.598);
  //sigma_e_b ~ inv_gamma(alpha_inv_gamma, 0.336);
  //sigma_G ~ inv_gamma(alpha_inv_gamma, 0.325);
  //sigma_pi_bar ~ inv_gamma(alpha_inv_gamma, 0.017);
  //sigma_er ~ inv_gamma(alpha_inv_gamma, 0.081);
  //sigma_ei ~ inv_gamma(alpha_inv_gamma, 0.085);
  //sigma_lamada_p ~ inv_gamma(alpha_inv_gamma, 0.16);
  //sigma_lamda_w ~ inv_gamma(alpha_inv_gamma, 0.289);
  //sigma_e_q ~ inv_gamma(alpha_inv_gamma, 0.604);
  //sigma_e_i ~ inv_gamma(alpha_inv_gamma, 3.52);
  //sigma_e_c ~ inv_gamma(alpha_inv_gamma, 0.336);
  // Beta priors for probability parameters
   rho ~ beta(0.3, 2.625);
  rho_el ~ beta(0.3, 2.625);
  rho_ea ~ beta(0.3, 2.625);
  rho_eb ~ beta(0.3, 2.625);
  rho_G ~ beta(0.3, 2.625);
  rho_pi ~ beta(0.3, 2.625);
  rh_i ~ beta(0.3, 2.625);
  rho_L ~ beta(0.3, 2.625);
  // Likelihood
  

  for (t in 2:T)  {
    real part1 = (h / (1 + h)) * consumption_growth_vector[t-1]; 
    real part2 = (1 / (1 + h)) * consumption_growth_vector[t-1]; 
    real part3 = ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * (hours_vector[t-1] - hours_vector[t]);
    real part4 = -((1 - h) / ((1 + h)) * sigma_c) * (ffr_vector[t] - inflation_vector[t-1]);
    real part5 = ((1 - h) / ((1 + h) * sigma_c)) * epsilon_C[t] ; // Assuming rho_c and eta_c are arrays generated in 'generated quantities'

    consumption_growth_vector[t] ~ normal(part1 + part2 + part3 + part4 + part5,18.22393);
     
     //r_k_star~ (-(kapital[t]-hours_vector[t])-wage_growth_vector[t],)
     //Output gap equation
 q[t] ~ normal(-(ffr_vector[t] - inflation_vector[t-1]) +
      ((1-tao) / (1 - tao + r_k_star[t])) * q[t-1] +
    (r_k_star[t] / (1 - tao + r_k_star[t])) * ffr_vector[t-1]+epsilon_Q[t],5.701086);
    investment_growth_vector[t] ~ normal((1 / (1 + Beta)) * investment_growth_vector[t-1]     +(Beta / (1 + Beta)) * investment_growth_vector[t-1] +
         ((1/phi_uppercase)/(1+Beta))*q[t]+epsilon_I[t],17.244);
         
   kapital[t] ~normal((1-tao)*kapital[t-1]+tao*investment_growth_vector[i]
   tao*epsilon_I[t],)
    
   hours_vector[t]~ normal(-wage_growth_vector[t] + (1+psi)*ffr_vector[t]+ kapital[t-1],)
   
   output_growth_vector[t] ~ normal((1 - (tao*ky) - gy) * consumption_growth_vector[t] + (tao*ky) * investment_growth_vector[t] + gy *epsilon_G[t],)
      
// Breaking down the calculation into parts
parta = (Beta / (1 + Beta * gamma_pi)) * (my_data$expected_Pi_hat_next[t-1]-my_data$pi_bar[t])
partb = (gamma_p / (1 + Beta * gamma_pi)) * (my_data$Pi_hat_lag1[i] -my_data$pi_bar[t]) 
partc =  (1/(1 - Beta * gamma_pi))* (((1-Beta*zeta_ps)*(1-zeta_ps))/zeta_ps)*(alpha*my_data$R_hat[t]+((1-alpha)*my_data$W_hat[t])-my_data$epsilon_pitwo[t]-(1-alpha)*gamma_w) 
parte =my_data$epsilon_Pi[t]
// Combine parts
 my_data$Pi_hat[i]~  normal(parta + partb + partc + parte + my_data$pi_bar[t])
    
    
  } 
}



```
```{r}
fit <- sampling(model, data =data_list , chains = 4, iter = 6000, warmup = 1000)

```

```{r}
my_data$I_hat[i] = (1 / (1 + Beta)) * my_data$I_hat_lag1[i] +   
   (Beta / (1 + Beta)) * my_data$expected_I_hat_next[i] +
         ((1/phi_uppercase)/(1+Beta))*my_data$Q_hat[i]+my_data$epsilon_I[i]
 
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
    
   my_data$L_hat[i] = -my_data$W_hat[i] + (1+psi)*my_data$R_hat[i]+ my_data$K_hat_lag1[i]
   
   my_data$Y_hat[i] = (1 - (tao*ky) - gy) * my_data$C_hat[i] + (tao*ky) * my_data$I_hat[i] + gy *my_data$epsilon_G[i]
  
   
# Breaking down the calculation into parts
parta <- (Beta / (1 + Beta * gamma_pi)) * (my_data$expected_Pi_hat_next[i-1]-my_data$pi_bar[i])
partb<- (gamma_p / (1 + Beta * gamma_pi)) * (my_data$Pi_hat_lag1[i] -my_data$pi_bar[i]) # Check if this should be [i] or [i-1]
partc <-  (1/(1 - Beta * gamma_pi))* (((1-Beta*zeta_ps)*(1-zeta_ps))/zeta_ps)*(alpha*my_data$R_hat[i]+((1-alpha)*my_data$W_hat[i])-my_data$epsilon_pitwo[i]-(1-alpha)*gamma_w) 
parte <- my_data$epsilon_Pi[i]
# Combine parts
 my_data$Pi_hat[i]<- parta + partb + partc + parte + my_data$pi_bar[i] 
 # Calculating individual parts of the equation
    part1r <- my_data$pi_bar[i]
    part2r <- rho * (my_data$R_hat_lag1[i] - my_data$Pi_bar_lag1[i])
   # part3r <- (1 - rho) * ((r_pi * (my_data$Pi_hat_lag1[i] - my_data$Pi_bar_lag1[i]))+(r_y*(my_data$Y_hat_lag1[i]-my_data$YP_hat_lag1[i])))
   part4r=r_pi_g*((my_data$Pi_hat[i]- my_data$pi_bar[i])-(my_data$Pi_hat_lag1[i] -my_data$Pi_bar_lag1[i]))
   #part5r=Ry_delta*((my_data$Y_hat[i]- my_data$YP_hat[i])-(my_data$Y_hat_lag1[i]- my_data$YP_hat_lag1[i]))
   part6r=my_data$epsilon_R[i]
    
    # Combining all parts to calculate R_hat for the current index
    my_data$R_hat[i] <- part1r + part2r + part4r  + part6r 
     # Taylor rule equation
 
    # Break down the equation into parts as before
  # Break down the equation into parts as before
    part1w <- (Beta / (1 + Beta)) * my_data$expected_W_hat_next[i]
    part2w <- (1 / (1 + Beta)) * my_data$W_hat_lag1[i]
    part3w <- (Beta / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    part4w <- ((1 + (Beta * gamma_w)) / (1 + Beta)) * (my_data$Pi_hat_lag1[i] - my_data$pi_bar[i])
    part5w <- (gamma_w / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    # Calculate the complex adjustment part
    adjustment_factor <- (1/(1+Beta))*((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws)
    complex_part <- adjustment_factor *( (my_data$W_hat_lag1[i] - (sigma_L * my_data$L_hat_lag1[i]) - (1 / (1 - h) )* (my_data$C_hat[i] -(h* my_data$C_hat_lag1[i]))  ))+my_data$epsilon_L[i] 
    # Combine all parts for the final calculation of W_hat
   my_data$W_hat[i] <- part1w + part2w + part3w - part4w + part5w - complex_part + my_data$epsilon_W[i]
      # Debugging code to print the value of each part after calculation
   
```
```{r}

library(actuar)
library(MASS)

```
```{r}
data_list <- list(
  output_growth_vector = final$Output_growth,
  consumption_growth_vector = final$Consumption_growth,
  investment_growth_vector = final$Investment_growth,
  wage_growth_vector = final$Wage_growth,
  hours_vector = final$Hours,
  inflation_vector = final$Inflation,
  ffr_vector = final$Ffr,
  kapital = my_data$K_hat,
  q = my_data$Q_hat
)

```


```{r}
library(MASS)  # Ensure MASS is loaded for fitdistr
```


```{r}
results <- list()  # Initialize results storage

# Loop through each data vector in the list
for (data_name in names(data_list)) {
    cat("Optimizing for", data_name, "\n")
    
    # Extract the current data vector
    current_data <- data_list[[data_name]]
    
    # Check if the current data vector is non-empty and numeric
    if (length(current_data) > 0 && is.numeric(current_data)) {
        # Fit the normal distribution to the current data vector
        fit_normal <- fitdistr(current_data, densfun = "normal")

        # Store the results
        results[[data_name]] <- fit_normal
        
        # Print the estimated parameters
        cat(data_name, "Estimated mean:", fit_normal$estimate["mean"], 
            "Estimated sd:", fit_normal$estimate["sd"], "\n")
    } else {
        cat(data_name, "data is not a non-empty numeric vector.\n")
    }
}

```
























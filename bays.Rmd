---
title: "baysmacro"
output: html_document
date: "2024-03-01"
---
```{r}

```

```{r}
Beta=.025
tao=0.025
alpha =0.3
psi=1/.169
gamma_p=.469
gamma_w=.763
lambda_w=.5
zeta_ps=.908
zeta_ws=0.737
sigma_L=2.4
sigma_c=1.353
h=0.573
phi=1.408
phi_uppercase=1/6.771
rbar_k=(1/Beta)-1+tao
Ky=8.8
sigma_eta_C<- 0.336
Invy=22
Cy=0.6
ky=Invy/tao
gy=1-Cy-Invy
r_pi=0.14
r_y=0.099
Ry_delta=0.159
rho=0.961
r_pi=1.684
rho_el=0.889
rho_ea=0.823
rho_eb=0.855
rho_G=0.949
rho_pi=0.924
 rh_i=0.927
rho_er=0
r_k_star <- (1 / Beta) - 1 + tao
rho_lamdaw=0
rho_q=0
rho_lamda_p=0
rho_L <-   0.9
sigma_e_L=3.52
sigma_e_a=.598
sigma_e_b=.336
sigma_G=.325
sigma_pi_bar=.017
sigma_er=0.081
sigma_ei=0.085
sigma_lamada_p=.16
sigma_lamda_w=.289
sigma_e_q=.604
sigma_e_i <- 3.52
sigma_e_c<- 0.336
alpha_k <- 0.3 
sigma_e_p <- 0.16 # Standard deviation for price markup shocks
sigma_e_w <- 0.289 # Standard deviation for wage markup shocks
sigma_e_q <- 0.604 # Standard deviation for equity price markup shocks


```
```{r}
# Define parameters for the distributions
mean_inverse_gamma <- 0.25
mean_inverse_gamma_pi_star <- 0.05
degrees_of_freedom <- 2
mean_beta <- 0.85
sd_beta <- 0.1

# For the Beta distribution parameters, convert mean and standard deviation to alpha and beta parameters
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)

# Define the Inverse-Gamma distribution for the standard errors of the innovations
sigma_e <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma * degrees_of_freedom))
sigma_e_pi_star <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma_pi_star * degrees_of_freedom))

# Convert from gamma to inverse gamma since R does not have a direct inverse gamma function
sigma_e <- 1 / sigma_e
sigma_e_pi_star <- 1 / sigma_e_pi_star

# Define the Beta distribution for the persistence parameters of the AR(1) processes
rho_ar1 <- rbeta(1, alpha_beta, beta_beta)
```
```{r}
# Define the number of observations
N <- 100 # Adjust N to match the length of your time series or dataset

# Assuming standard deviations for the shocks are specified or have been determined based on your model
# If not specified, you need to define them. Here are example values:
sigma_e_p <- 0.16 # Standard deviation for price markup shocks
sigma_e_w <- 0.289 # Standard deviation for wage markup shocks
sigma_e_q <- 0.604 # Standard deviation for equity price markup shocks

# Generate the IID white noise processes for the markup shocks
epsilon_p <- rnorm(N, mean = 0, sd = sigma_e_p) # Price markup shocks
epsilon_w <- rnorm(N, mean = 0, sd = sigma_e_w) # Wage markup shocks
epsilon_q <- rnorm(N, mean = 0, sd = sigma_e_q) # Equity price markup shocks

```
```{r}
calculate_ema_4th_order <- function(data, n=4) {
  # Ensure data is in correct format (numeric vector)
  if (!is.numeric(data)) {
    stop("Data must be a numeric vector.")
  }

  alpha <- 2 / (n + 1)
  ema <- numeric(length(data))
  ema[1] <- data[1]  # Initialize the first EMA value to the first data point
  
  for (t in 2:length(data)) {
    ema[t] <- alpha * data[t] + (1 - alpha) * ema[t-1]
  }
  
  # To calculate expected next value, we simply apply the formula one more time assuming last value repeats
  expected_next_ema <- alpha * data[length(data)] + (1 - alpha) * ema[length(ema)]
  return(list(ema = ema, expected_next = expected_next_ema))
}



# Now, apply the calculate_ema_4th_order function to this lagged series
# Assuming you have initialized your dataframe and it's being updated elsewhere in your simulation

calculate_ema_4th_order_for_df <- function(df, column_name, n=4) {
  # Extract the column data based on column_name
  data <- df[[column_name]]
  
  # Filter out NA values that arise due to lagging
  data <- na.omit(data)
  
  # Apply the EMA calculation
  ema_result <- calculate_ema_4th_order(data, n)
  
  # Return the result
  return(ema_result)
}


```


```{r}
# Define the parameters for the Beta distribution based on the given mean and standard deviation
mean_beta <- 0.85
sd_beta <- 0.1
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)

# Simulate a persistence parameter from the Beta distribution
rho_C <- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_c<- 0.336  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_c <- rnorm(N, mean = 0, sd = sigma_eta_c)

# Initialize your series
epsilon_c <- numeric(N)
epsilon_c[1] <- eta_c[1]  # You could start with the first shock or with a different initial value

# Simulate the AR(1) process
for(t in 2:N) {
  epsilon_c[t] <- rho_c * epsilon_c[t-1] + eta_c[t]
}

# epsilon_l now contains the AR(1) process values

```
```{r}
# Initialize your new dataframe
data <- data.frame(
  time = 1:4,  # Assuming you need a 'time' variable for arranging
  C_hat = c(0, 0, 0, 0),
  I_hat = c(0, 0, 0, 0),
  R_hat = c(0, 0, 0, 0)  # Add R_hat if you need it, initializing with zeros or appropriate values
)

```



```{r}
library(dplyr)

# Assuming 'data' is a dataframe that contains columns C_hat, i_hat, R_hat, and e_b_hat
# which represent your time series data.

# Add lagged and expected (lead) values to the dataframe
data <- data %>%
  arrange(time) %>% # Replace 'time' with your actual time variable
  mutate(
    C_hat_lag1[t] == lag(C_hat, 1),
    i_hat_lag1[t]== lag(i_hat, 1),
    expected_C_hat_next == calculate_ema_4th_order(C_hat)$expected_next,
    expected_R_hat_next == calculate_ema_4th_order(R_hat)$expected_next, # This is a placeholder for actual expected values
  )

# Calculate the new C_hat using the lagged and expected values
data <- data %>%
  mutate(
    C_hat_new[t] = h / (1 + h) * C_hat_lag1 +
                1 / (1 + h) * expected_C_hat_next +
                (sigma_c - 1) / (sigma_c * (1 + lambda) * (1 + h)) * (i_hat - i_hat_lag1) -
                (1 - h) / ((1 + h) * phi * sigma_c) * (R_hat - expected_R_hat_next) +
                (1 - h) / ((1 + h) * sigma_c) * epsilon_c[t]
  )

```
```{r}

# Simulate a persistence parameter from the Beta distribution
rho_Q<- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_Q<- 0.604  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_Q <- rnorm(N, mean = 0, sd = sigma_eta_Q)

# Initialize your series
epsilon_Q <- numeric(N)
epsilon_Q[1] <- eta_Q[1]  # You could start with the first shock or with a different initial value

```

```{r}

# Calculate the adjusted rental rate of capital (r_k_star)
r_k_star <- (1 / Beta) - 1 + tao

# Add expected future value of the output gap to the dataframe
data <- data %>%
  mutate(
    Q_hat_next_expected[t] = ((lag(Q_hat,1)-lag(Q_hat, 2))+ lag(Q_hat,1)),  # expected future value of the output gap
    r_k_next_expected[t]=((lag(r_k,1)-lag(r_k, 2))+ lag(r_k,1)),      # expected future rental rate of capital
    
    # Output gap equation
    Q_hat[t] = -(R_hat - pi_e_next) +
            1-tau / (1 - tau + r_k_star) * Q_hat_next_expected +
            r_k / (1 - tau + r_k_star) * r_k_next_expected + epsilon_q[t]
          
  )

```

```{r}

# Simulate a persistence parameter from the Beta distribution
rho_I <- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_I <- 3.52  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_I <- rnorm(N, mean = 0, sd = sigma_eta_I)


```

```{r}
# Calculate the new i_hat using the lagged, lead, and current values
data <- data %>%
  arrange(time) %>% # Replace 'time' with your actual time variable if necessary
  mutate(
    i_hat_lag1[t] = lag(i_hat, 1),                    # lagged i_hat
    E_i_hat_next[t] = ((lag(i_hat,1)-lag(i_hat, 2))+ lag(i_hat,1)),                 # expected i_hat next period
    # The following line is a placeholder for your epsilon_i calculation
    i_hat_new[t] = (1 / (1 + Beta)) * i_hat_lag1 +    # recursive equation part
                (Beta / (1 + Beta)) * E_i_hat_next +
                (1 / (1 + Beta)) * (1 / phi_uppercase) * Q_hat +epsilon_i[t] 
                
  )
```
```{r}

# Simulate a persistence parameter from the Beta distribution
rho_K <- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_K <- 1  # Replace with the actual standard deviation of your white noise process


```


```{r}

# Constants
tau <- 0.025  # Assuming a depreciation rate

# Calculate the capital stock K_hat
data <- data %>%
  mutate(
    K_hat_lag[t]= lag(K_hat, 1),             # capital stock in the previous period
    I_hat_lag[t] = lag(I_hat, 1),             # investment in the previous period
    epsilon_I_lag[t]= lag(epsilon_I, 1),     # investment shock in the previous period
    K_hat[t] = (1 - tau) * K_hat_lag + tauI_hat_lag + tau *  epsilon_k[t]
  )

# Handle the initial value for K_hat as it won't have a lagged value available

```


```{r}

# Simulate a persistence parameter from the Beta distribution
rho_w<- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_w <- 1  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_w <- rnorm(N, mean = 0, sd = sigma_eta_w)

# Initialize your series
epsilon_w <- numeric(N)
epsilon_w[1] <- eta_w[1]  # You could start with the first shock or with a different initial value

# Simulate the AR(1) process
for(t in 2:N) {
  epsilon_w[t] <- rho_w * epsilon_w[t-1] + eta_w[t]
}

```


```{r}
library(dplyr)

# Constants for the coefficients
Beta <- 0.99    # Discount factor
gamma_w <- 0.763  # Weight on lagged wage inflation
zatza_ws <- 0.6      # Slope parameter of the wage Phillips curve
lambda_w <- 0.5  # Weight on labor productivity
h <- 0.573       # Habit formation parameter in consumption
data <- data %>%
  arrange(time) %>%  # Make sure the data is in time order
  mutate(
    w_hat_t_lag1[t] = lag(W_hat, 1),
    w_hat_t_plus1_expected[t] = lag(W_hat, 1) - lag(W_hat, 2) + lag(W_hat, 1),
    pi_hat_t_plus1_expected[t] = lag(pi_hat_t, 1) - lag(pi_hat_t, 2) + lag(pi_hat_t, 1),
    pi_hat_t_lag1[t] = lag(pi_hat_t, 1),
    C_hat_t_lag1[t]= lag(C_hat_t, 1),
    pi_star[t] = pi_hat_t,  # Assuming current period's inflation is the target, replace with actual target if different

    # Wage-setting equation using the new definitions and provided variable names
    w_hat_t_new[t] = (Beta / (1 + Beta)) * w_hat_t_plus1_expected +
                  (1 / (1 + Beta)) * w_hat_t_lag1 +
                  (Beta / (1 + Beta)) * (pi_hat_t_plus1_expected - pi_star) +
                  ((1+(Beta*gamma_w ))/ (1 + Beta)) * (pi_hat_t_lag1-pi_star) -
                  (1 / (1 + Beta)) * ((1 - (Beta * zatza_ws) * (1 - zatza_ws))/ 
                    (1 - ((1 - lambda_w) * sigma_L) / lambda_w) * zatza_ws) *
                    (w_hat_t_lag1 - sigma_L * L_hat_t - 
                     (1 / (1 - h)) * (C_hat_t - h * C_hat_t_lag1) +
                     my_data$e[t]) + epsilon_w[t]# eta_w_t needs to be defined or calculated
  )

```
```{r}

# Simulate a persistence parameter from the Beta distribution
rho_pi <- rbeta(1, alpha_beta, beta_beta)

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_pi <- 1  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_pi<- rnorm(N, mean = 0, sd = sigma_eta_pi)

# Initialize your series
epsilon_pi <- numeric(N)
epsilon_pi[1] <- eta_l[1]  # You could start with the first shock or with a different initial value

# Simulate the AR(1) process
for(t in 2:N) {
  epsilon_pi[t] <- rho_pi * epsilon_pi[t-1] + eta_pi[t]
}
```

```{r}

# Constants for the coefficients
Beta <- 0.99    # Discount factor
gamma_p <- 0.469  # Weight on lagged inflation
xi_p <- 0.6      # Slope parameter of the Phillips curve
alpha_k <- 0.3  # replace with actual value from your model

# Add lagged and expected future values to the dataframe
data <- data %>%
  arrange(time) %>% # Replace 'time' with your actual time variable
  mutate(
    pi_hat_t_lag1[t] = lag(pi_hat_t, 1),
    E_pi_hat_t_plus1[t] = ((pi_hat_t_lag1-lag(pi_hat_t, 2))+ pi_hat_t_lag1), # Expected future inflation

    # Phillips curve equation
    pi_hat_t_new[t] = (Beta / (1 + Beta * gamma_p)) * (E_pi_hat_t_plus1 - pi_star) +(gamma_p / (1 + Beta * gamma_p)) * (pi_hat_t_lag1 - pi_star) +  (1 / (1 + Beta * gamma_p)) * (1 - Beta * zata_ps) * (1 - zata_ps) / zata_ps *(alpha_k * k_t_hat + (1 - alpha_k) * w_t_hat - epsilon_pi[t] - (1 - alpha_k) * gamma_p)+epsilon_p[t]
  )
```

```{r}
# rho_L is the autoregressive coefficient for the labor shock process
rho_L <- 0.9  # Replace with the actual estimated value for rho_L

# eta_L represents the stochastic shock at time t for the labor process
# This should be a vector of shocks. Here's how you might simulate it for demonstration:
eta_L <- rnorm(N, mean = 0, sd = 1)  # N is the length of your time series

# Initialize your labor shock series with an initial value
epsilon_L <- numeric(N)
epsilon_L[1] <- eta_L[1]  # Starting with the first shock, or you could start with a different initial value

# Simulate the AR(1) process
for(t in 2:N) {
  epsilon_L[t] <- rho_L * epsilon_L[t-1] + eta_L[t]
}

# Now epsilon_L contains the labor shock process values

```
```{r}
# Assuming you have defined the variables w_hat_t (wage deviation), psi (inverse elasticity),
# r_k_hat_t (rental rate of capital deviation), and K_hat_lag (lagged capital deviation)

# Calculate L_hat_t based on the given formula
L_hat_t[t] <- -w_hat_t + (1 + psi) * r_k_hat_t + K_hat_lag

# Note that you will need actual time series data for w_hat_t, r_k_hat_t, and K_hat_lag.
# Also, psi should be a predefined constant in your model.

```

```{r}
library(dplyr)

# Assuming 'data' is a dataframe that contains columns:
# w_hat_t for real wage
# r_k_hat_t for rental rate of capital
# K_hat_t_lag1 for lagged capital stock

# You would also need to define or estimate the value of psi (ψ) from your model
psi <- 1.5 # Example value, replace with the actual value from your model

data <- data %>%
  arrange(time) %>% # Replace 'time' with your actual time variable
  mutate(
    K_hat_t_lag1[t] = lag(K_hat_t, 1), # Create lagged capital stock variable
    L_hat_t[t] = -w_hat_t + (1 + psi) * r_k_hat_t + K_hat_t_lag1
  )

# After running this code, data will now include the new column L_hat_t with the calculated labor.

```
```{r}
library(dplyr)

# Define parameters (These should be given specific values based on your model)
delta <- 0.025  # Depreciation rate
sy <- 0.3       # Steady-state capital-output ratio
sg <- 0.2       # Steady-state government spending-output ratio
phi <- 1.05     # Fixed cost adjustment
rho_G <- 0.9    # AR(1) coefficient for government spending shock
sigma_eta_G <- 0.2  # Standard deviation of the shock (for simulation)

# Assuming 'data' is a dataframe that contains columns for capital (k_t_lag1),
# investment (i_t), government spending (g_t), and labor (L_t)
# We also need the shock epsilon_G, which we'll simulate here as an example

# Simulate government spending shock with an AR(1) process
N <- nrow(data)  # Number ofrhows in your dataframe
eta_G <- rnorm(N, mean = 0, sd = sigma_eta_G)  # Innovation term for government spending shock
epsilon_G <- numeric(N)
epsilon_G[1] <- eta_G[1]  # Initial value of the shock

# Calculate the AR(1) process for the government spending shock
for (t in 2:N) {
  epsilon_G[t] <- rho_G * epsilon_G[t-1] + eta_G[t]
}


```
```{r}
# Assuming that 'data' is a dataframe that contains the columns 'C_hat_t' for consumption deviation, 
# 'I_hat_t' for investment deviation, and 'epsilon_G_t' for government spending shock.
# Also, 'pi_k' and 'g_s' are constants that need to be defined.

# Define the constants 'pi_k' and 'g_s'
pi_k <- 0.3  # Replace with the actual value for pi_k
g_s <- 0.2   # Replace with the actual value for g_s

# Compute Y_hat_t
data <- data %>%
  mutate(
    Y_hat_t[t] = (1 - pi_k - g_s) * C_hat_t + pi_k * I_hat_t + g_s *epsilon_G[t]
  )

```

```{r}
library(dplyr)

# Assuming that 'data' is a dataframe that contains the columns w_hat_t, r_k_hat_t, and K_hat_lag1,
# and psi is already defined in your environment.
# Here's how you'd calculate L_hat_t according to your equation:

data <- data %>%
  mutate(
    L_hat_t[t] = -w_hat_t + (1 + psi) * r_k_hat_t + K_hat_lag1
  )

```
```{r}

# Define the length of your time series
N <- 100  # Replace with your actual sample size

# Define the standard deviation for the white noise shock
# This should be known or estimated from your data; here I'll assume a placeholder value
sigma_eta_pi_star <- 1  # Replace with the actual standard deviation of your white noise process

# Generate an IID white noise shock series
eta_pi_star <- rnorm(N, mean = 0, sd = sigma_eta_pi_star)

# Initialize your series
epsilon_pi_star <- numeric(N)
epsilon_pi_star[1] <- eta_pi_star[1]  # You could start with the first shock or with a different initial value

# Simulate the AR(1) process
for(t in 2:N) {
  epsilon_pi_star[t] <- rho_pi_star * epsilon_pi_star[t-1] + eta_pi_star[t]
}

```
```{r}

data <- data %>%
  mutate(
    pi_hat_lag[t] = lag(pi_hat_t),
    pi_star_lag[t]= lag(pi_star_t),
    Y_hat_lag[t] = lag(Y_hat_t),
    Y_star_lag[t]= lag(Y_star),

    # Taylor rule equation
    R_hat_t[t] = pi_star_t + 
              rho * (R_hat_lag - pi_star_lag) +
              (1 - rho) * (r_pi * (pi_hat_t - pi_star_lag) + r_Y * (Y_hat_t - Y_star_lag)) +
              r_pi * (pi_hat_lag - pi_star_lag) +
              r_Y * (Y_hat_lag - Y_star_lag) + epsilon_pi_star[t]
              )
```
```{r}
calculate_running_average <- function(data, n = 3) {
  # data: Numeric vector
  # n: window size for the moving average
  
  # Calculate the running average
  running_avg <- stats::filter(data, rep(1/n, n), sides = 2)
  
  return(running_avg)
}

# Example usage
data_vector <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
running_avg_result <- calculate_running_average(data_vector, 3)
print(running_avg_result)
library(zoo)

```

```{r}
# Generate the IID white noise processes for the markup shocks
epsilon_p <- rnorm(N, mean = 0, sd = sigma_e_p) # Price markup shocks
epsilon_w <- rnorm(N, mean = 0, sd = sigma_e_w) # Wage markup shocks
epsilon_q <- rnorm(N, mean = 0, sd = sigma_e_q) # Equity price markup shocks

```
```{r}
calculate_next_period_with_average_diff <- function(data) {
  # Ensure there are at least 5 data points
  if (length(data) < 5) {
    stop("Insufficient data. Need at least 5 data points.")
  }
  
  # Calculate the differences of the last 5 data points
  diffs <- diff(tail(data, 5))
  
  # Calculate the average difference
  avg_diff <- mean(diffs)
  
  # Predict the next period by adding the average difference to the last data point
  next_period <- tail(data, 1) + avg_diff
  
  return(next_period)
}

```

```{r}
# Initialize the dataframe with the first 4rhows set to 1
my_data <- data.frame(
    time = 1:104,
    C_hat = rep(10000, 104),
    L_hat=rep(10000,104),
    W_hat = rep(10000, 104),
    I_hat = rep(10000, 104),
     C_hat_lag1 = rep(10000, 104),  # Lagged values for the firstrhow would be NA
    I_hat_lag1 = rep(10000, 104),
     expected_I_hat_next=rep(10000,104),
     expected_W_hat_next=rep(500,104),
    K_hat=rep(10000,104),
    Pi_hat=rep(1,104),
    Y_hat=rep(4000,104),
    R_hat = rep(7,104),
    W_hat=rep(500,104),
    R_k=rep(1,104),
    Q_hat = rep(10000,104),
     pi_bar=rep(1,104),
    epsilon_C = rep(1, 104),
    epsilon_Q = rep(1, 104),
     epsilon_I = rep(1, 104),
    epsilon_W = rep(1, 104),
    epsilon_R = rep(1, 104),
    epsilon_Pi=rep(1,104),
    epsilon_L=rep(1,104),
    epsilon_G=rep(1,104),
     epsilon_R=rep(1,104),
     epsilon_pitwo=rep(1,104),
    expected_RK_hat_next=rep(10000,104),
    expected_L_hat_next=rep(10000,104),
    expected_C_hat_next=rep(10000,104),
    expected_Q_hat_next = rep(1000, 104),
    expected_Pi_hat_next=rep(1,104),
    expected_R_hat_next = rep(7, 104),
   # Lagged values for the firstrhow would be NA
    Q_hat_lag1 = rep(10000, 104),
    W_hat_lag1=rep(500,104),
     R_hat_lag1 = rep(7,104),
    R_k_hat_lag1=rep(1,104),
    L_hat_lag1 = rep(7,104),
    Y_hat_lag1=rep(1,104),
    YP_hat_lag1=rep(1,104),
    Pi_bar_lag1=rep(1,104),
     Pi_hat_lag1=rep(1,104),
    K_hat_lag1 = rep(10000,104)
)
my_data$YP_hat=my_data$Pi_hat/my_data$W_hat
```
```{r}
# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 6:104) {
    # Update lagged values
    my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
    my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
    my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
    my_data$R_k_hat_lag1[i] <- my_data$R_k[i-1]
    my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
    my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
    my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
    my_data$Pi_bar_lag1[i] <- my_data$pi_bar[i-1]
    my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
    my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
     my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
    
    
     # Generate new shock value for epsilon_c for the currentrhow
    eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
    
# Simulate a persistence parameter from the Beta distribution
    rho_c<- rbeta(1, alpha_beta, beta_beta)
     my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
       # Generate new shock value for epsilon_q for the current row
     epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
       my_data$epsilon_Q[i] <- epsilon_Q
       eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
       rho_q<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
      
       epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
      epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
      my_data$epsilon_Pi[i]=epsilon_p
      
        rho_pi<- rbeta(1, alpha_beta, beta_beta)
        eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
       my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
        eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
        rho_g<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_Pi[i] <- rho_g * my_data$epsilon_pitwo[i-1] + eta_g
       epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
       
       
    # Ensure the calculate_ema_4th_order function is defined to return a list with expected_next
      my_data$expected_C_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$C_hat[(i-5):(i-1)])
        my_data$expected_I_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$I_hat[(i-5):(i-1)])
        my_data$expected_Q_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$Q_hat[(i-5):(i-1)])
        my_data$expected_W_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$W_hat[(i-5):(i-1)])
        my_data$expected_Pi_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$Pi_hat[(i-5):(i-1)])
        my_data$expected_R_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$R_hat[(i-5):(i-1)])
         my_data$expected_L_hat_next[i-1] <- calculate_next_period_with_average_diff(my_data$L_hat[(i-5):(i-1)])
     # Calculate running averages up to the current index
    if (i > 5) {  # Ensure there are enough data points to calculate a running average
        window_size <- i  # Define your window size for the running average
        # Calculate running average for C_hat using values up to the current index
        my_data$pi_bar[i] <- tail(rollapply(my_data$Pi_hat[1:i], width = window_size, FUN = mean, align = "right", fill = 1), 1)
        
    
    
part1 <- (h / (1 + h)) * my_data$C_hat_lag1[i]
part2 <- (1 / (1 + h)) * my_data$expected_C_hat_next[i]
part3 <- ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * ( my_data$L_hat[i]-my_data$expected_L_hat_next[i-1])
part4 <- -((1 - h) / ((1 + h)) * sigma_c) * (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i-1])
part5 <- ((1 - h) / ((1 + h) * sigma_c)) * my_data$epsilon_C[i]

# Combine all parts to calculate C_hat for the current index
my_data$C_hat[i] <- part1 + part2 + part3 + part4 + part5

  

        
  # Output gap equation
my_data$Q_hat[i] = -(my_data$R_hat[i] - my_data$expected_Pi_hat_next[i-1]) +
       ((1-tao) / (1 - tao + r_k_star)) * my_data$expected_Q_hat_next[i-1] +
        (r_k_star / (1 - tao + r_k_star)) * my_data$expected_R_hat_next[i-1] + my_data$epsilon_Q[i]

my_data$I_hat[i] = (1 / (1 + Beta)) * my_data$I_hat_lag1[i] +   
   (Beta / (1 + Beta)) * my_data$expected_I_hat_next[i-1] +
         ((1/phi_uppercase)/(1+Beta))*my_data$Q_hat[i]+my_data$epsilon_I[i]
 
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
    
   my_data$L_hat[i] = -my_data$W_hat[i] + (1 + psi) *my_data$R_hat[i]+ my_data$K_hat_lag1[i]
   
   my_data$Y_hat[i] = (1 - (tao*ky) - gy) * my_data$C_hat[i] + (tao*ky) * my_data$I_hat[i] + gy *my_data$epsilon_G[i]
  
   
# Breaking down the calculation into parts
parta <- (Beta / (1 + Beta * gamma_p)) * (my_data$expected_Pi_hat_next[i-1]-my_data$pi_bar[i])
partb<- (gamma_p / (1 + Beta * gamma_p)) * (my_data$Pi_hat_lag1[i] -my_data$pi_bar[i]) # Check if this should be [i] or [i-1]
partc <-  (1/(1 - Beta * gamma_p))* (((1-Beta*zeta_ps)*(1-zeta_ps))/zeta_ps)*(alpha*my_data$R_hat[i]+((1-alpha)*my_data$W_hat[i])-my_data$epsilon_pitwo[i]-(1-alpha)*gamma_w) 
parte <- my_data$epsilon_Pi[i]
 

# Combine parts
 my_data$Pi_hat[i]<- parta + partb + partc + parte + my_data$pi_bar[i] 

    # Debugging code to print the value of each part after calculation
    print(paste("Iteration:", i))
    print(paste("Parta:", parta))
    print(paste("Partb:", partb))
    print(paste("Partc:", partc))
    print(paste("Parte:", part4))
    print(paste("Pibar:", part4))
    print(paste("pi_hat[i]:", my_data$Pi_hat[i]))
 
    # Taylor rule equation
   #my_data$R_hat[i] = my_data$pi_bar[i] + 
            # rho * (my_data$R_hat_lag1[i] - my_data$Pi_bar_lag1[i]) +
           #(1 - rho) * (r_pi * (my_data$Pi_hat_lag1[i] - my_data$Pi_bar_lag1[i])) + (r_pi * (my_data$Y_hat_lag1[i] - my_data$YP_hat_lag1[i]))+
           #  ( r_pi * (my_data$Pi_hat[i] - my_data$pi_bar[i])-(my_data$Pi_hat_lag1[i]-my_data$Pi_bar_lag1[i])) +(r_y * (my_data$Y_hat[i] -my_data$YP_hat[i])-(my_data$Y_hat_lag1[i]-my_data$YP_hat_lag1[i]) ) + my_data$epsilon_R[i]


    # Break down the equation into parts as before
  # Break down the equation into parts as before
    part1 <- (Beta / (1 + Beta)) * my_data$expected_W_hat_next[i-1]
    part2 <- (1 / (1 + Beta)) * my_data$W_hat_lag1[i]
    part3 <- (Beta / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    part4 <- - ((1 + (Beta * gamma_w)) / (1 + Beta)) * (my_data$Pi_hat_lag1[i] - my_data$pi_bar[i])
    part5 <- (gamma_w / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])

    # Calculate the complex adjustment part
    adjustment_factor <- -1*(1/(1+Beta))*((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws)
    complex_part <- adjustment_factor * (my_data$W_hat_lag1[i] - sigma_L * my_data$L_hat[i] - (1 / (1 - h)) * (my_data$C_hat[i] - (my_data$expected_C_hat_next[i-1] * h))) + my_data$epsilon_L[i]

    # Combine all parts for the final calculation of W_hat
    #my_data$W_hat[i] <- part1 + part2 + part3 + part4 + part5 - complex_part + my_data$epsilon_W[i]
      # Debugging code to print the value of each part after calculation
   
 

    
    }
}
  
```


```{r}
# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 5:104) {
    # Update lagged values
    my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
    my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
    my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
    my_data$R_k_hat_lag1[i] <- my_data$R_k[i-1]
    my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
    my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
    my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
    my_data$Pi_bar_lag1[i] <- my_data$pi_bar[i-1]
    my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
    my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
    
    
     # Generate new shock value for epsilon_c for the currentrhow
    eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
    
# Simulate a persistence parameter from the Beta distribution
    rho_c<- rbeta(1, alpha_beta, beta_beta)
     my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
       # Generate new shock value for epsilon_q for the current row
     epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
       my_data$epsilon_Q[i] <- epsilon_Q
       eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
       rho_q<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
      eta_L<- rnorm(1, mean = 0, sd = sigma_e_L)
      rho_L<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_L[i] <- rho_L * my_data$epsilon_L[i-1] + eta_L
       epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
      epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
      my_data$epsilon_Pi[i]=epsilon_p
       my_data$epsilon_L[i] <- rho_L * my_data$epsilon_L[i-1] + eta_L
        rho_pi<- rbeta(1, alpha_beta, beta_beta)
        eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
       my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
        eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
        rho_g<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_pitwo[i] <- rho_g * my_data$epsilon_pitwo[i-1] + eta_g
        epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
       
       
    # Ensure the calculate_ema_4th_order function is defined to return a list with expected_next
    my_data$expected_C_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$C_hat[1:(i-1)]), 4)$expected_next
    my_data$expected_I_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$I_hat[1:(i-1)]), 4)$expected_next
    my_data$expected_Q_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$Q_hat[1:(i-1)]), 4)$expected_next
    my_data$expected_W_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$W_hat[1:(i-1)]), 4)$expected_next
    my_data$expected_Pi_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$Pi_hat[1:(i-1)]), 4)$expected_next
    my_data$expected_R_hat_next[i-1] <- calculate_ema_4th_order(na.omit(my_data$R_hat[1:(i-1)]), 4)$expected_next
    # Add other expected next value calculations as necessary
     # Calculate running averages up to the current index
    if (i > 5) {  # Ensure there are enough data points to calculate a running average
        window_size <- i  # Define your window size for the running average
        # Calculate running average for C_hat using values up to the current index
        my_data$pi_bar[i] <- tail(rollapply(my_data$Pi_hat[1:i], width = window_size, FUN = mean, align = "right", fill = 1), 1)
        
    
    
part1 <- (h / (1 + h)) * my_data$C_hat_lag1[i]
part2 <- (1 / (1 + h)) * my_data$expected_C_hat_next[i-1]
part3 <- ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * ( my_data$I_hat[i]-my_data$expected_I_hat_next[i-1])
part4 <- -((1 - h) / ((1 + h)) * sigma_c) * (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i-1])
part5 <- ((1 - h) / ((1 + h) * sigma_c)) * my_data$epsilon_C[i]

# Combine all parts to calculate C_hat for the current index
my_data$C_hat[i] <- part1 + part2 + part3 + part4 + part5
   

        
  # Output gap equation
my_data$Q_hat[i] = -(my_data$R_hat[i] - my_data$expected_Pi_hat_next[i-1]) +
        ((1-tao) / (1 - tao + r_k_star)) * my_data$expected_Q_hat_next[i-1] +
        (r_k_star / (1 - tao + r_k_star)) * my_data$expected_R_hat_next[i-1] + my_data$epsilon_Q[i]

 my_data$I_hat[i] = (1 / (1 + Beta)) * my_data$I_hat_lag1[i] +   
   (Beta / (1 + Beta)) * my_data$expected_I_hat_next[i-1] +
          ((1/phi_uppercase)/(1+Beta))*my_data$Q_hat[i]+my_data$epsilon_I[i]
 
  
   my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   +tao*my_data$epsilon_I[i-1]
    
   my_data$L_hat[i] = -my_data$W_hat[i] + (1 + psi) *my_data$R_hat[i]+ my_data$K_hat_lag1[i]
   
   my_data$Y_hat[i] = (1 - (tao*ky) - gy) * my_data$C_hat[i] + (tao*ky) * my_data$I_hat[i] + gy *my_data$epsilon_G[i]
  
   
# Breaking down the calculation into parts
parta <- (Beta / (1 + Beta * gamma_p)) * (my_data$expected_Pi_hat_next[i-1]-my_data$pi_bar[i])
partb<- (gamma_p / (1 + Beta * gamma_p)) * (my_data$Pi_hat_lag1[i] -my_data$pi_bar[i]) # Check if this should be [i] or [i-1]
partc <- 1/(1 - Beta * gamma_p)* (((1-Beta*zeta_ps)*(1-zeta_ps))/zeta_ps)*(alpha*my_data$R_hat[i]+(1-alpha)*my_data$W_hat[i]-my_data$epsilon_pitwo[i]-(1-alpha)*gamma_w) 
parte <- my_data$epsilon_Pi[i]
 

# Combine parts
 my_data$Pi_hat[i]<- parta + partb + partc + parte + my_data$pi_bar[i] 

    
 
    # Taylor rule equation
   my_data$R_hat[i] = my_data$pi_bar[i] + 
              rho * (my_data$R_hat_lag1[i] - my_data$Pi_bar_lag1[i]) +
              (1 - rho) * (r_pi * (my_data$Pi_hat_lag1[i] - my_data$Pi_bar_lag1[i])) + (r_pi * (my_data$Y_hat_lag1[i] - my_data$YP_hat_lag1[i]))+
             ( r_pi * (my_data$Pi_hat[i] - my_data$pi_bar[i])-(my_data$Pi_hat_lag1[i]-my_data$Pi_bar_lag1[i])) +(r_y * (my_data$Y_hat[i] -my_data$YP_hat[i])-(my_data$Y_hat_lag1[i]-my_data$YP_hat_lag1[i]) ) + my_data$epsilon_R[i]


    # Break down the equation into parts as before
    part1 <- (Beta / (1 + Beta)) * my_data$expected_W_hat_next[i-1]
    part2 <- (1 / (1 + Beta)) * my_data$W_hat_lag1[i]
    part3 <- (Beta / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    part4 <- - ((1 + (Beta * gamma_w)) / (1 + Beta)) * (my_data$Pi_hat_lag1[i] - my_data$pi_bar[i])
    part5 <- (gamma_w / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])

    # Calculate the complex adjustment part
    adjustment_factor <- -1*(1/(1+Beta))*((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws)
    complex_part <- adjustment_factor * (my_data$W_hat_lag1[i] - sigma_L * my_data$L_hat[i] - (1 / (1 - h)) * (my_data$C_hat[i] - (my_data$expected_C_hat_next[i-1] * h))) + my_data$epsilon_L[i]

    # Combine all parts for the final calculation of W_hat
    my_data$W_hat[i] <- part1 + part2 + part3 + part4 + part5 - complex_part + my_data$epsilon_W[i]
     
 

    
    }
}

    


```

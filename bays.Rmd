---
title: "baysmacro"
output: html_document
date: "2024-03-01"
---

```{r}
Beta=.025
tao=0.025
alpha =0.3
psi=1/.169
gamma_p=.469
gamma_w=.7111
lambda_w=.5
zeta_ps=.908
zeta_ws=0.737
sigma_L=2.4
sigma_c=1.353
h=0.573
phi=1.408
phi_uppercase=1/6.771
rbar_k=(1/Beta)-1+tao
Ky=8.8
sigma_eta_C<- 0.336
Invy=22
Cy=0.6
Ky=Invy/tao
gy=1-Cy-Invy
r_pi_g=0.14
r_y=0.099
Ry_delta=0.159
rho=0.961
r_pi=1.684
rho_el=0.889
rho_ea=0.823
rho_eb=0.855
rho_G=0.949
rho_pi=0.924
 rh_i=0.927
rho_er=0
r_k_star <- (1 / Beta) - 1 + tao
rho_lamdaw=0
rho_q=0
rho_lamda_p=0
rho_L <-   0.9
sigma_e_L=3.52
sigma_e_a=.598
sigma_e_b=.336
sigma_G=.325
sigma_pi_bar=.017
sigma_er=0.081
sigma_ei=0.085
sigma_lamada_p=.16
sigma_lamda_w=.289
sigma_e_q=.604
sigma_e_i <- 3.52
sigma_e_c<- 0.336
alpha_k <- 0.3 
sigma_e_p <- 0.16 # Standard deviation for price markup shocks
sigma_e_w <- 0.289 # Standard deviation for wage markup shocks
sigma_e_q <- 0.604 # Standard deviation for equity price markup shocks


```
```{r}
# Define parameters for the distributions
mean_inverse_gamma <- 0.25
mean_inverse_gamma_pi_star <- 0.05
degrees_of_freedom <- 2
mean_beta <- 0.85
sd_beta <- 0.1

# For the Beta distribution parameters, convert mean and standard deviation to alpha and beta parameters
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)

# Define the Inverse-Gamma distribution for the standard errors of the innovations
sigma_e <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma * degrees_of_freedom))
sigma_e_pi_star <- rgamma(1, shape = 1 / degrees_of_freedom, scale = 1 / (mean_inverse_gamma_pi_star * degrees_of_freedom))

# Convert from gamma to inverse gamma since R does not have a direct inverse gamma function
sigma_e <- 1 / sigma_e
sigma_e_pi_star <- 1 / sigma_e_pi_star

# Define the Beta distribution for the persistence parameters of the AR(1) processes
rho_ar1 <- rbeta(1, alpha_beta, beta_beta)
```

```{r}
calculate_ema_4th_order <- function(data, n=4) {
  # Ensure data is in correct format (numeric vector)
  if (!is.numeric(data)) {
    stop("Data must be a numeric vector.")
  }

  alpha <- 2 / (n + 1)
  ema <- numeric(length(data))
  ema[1] <- data[1]  # Initialize the first EMA value to the first data point
  
  for (t in 2:length(data)) {
    ema[t] <- alpha * data[t] + (1 - alpha) * ema[t-1]
  }
  
  # To calculate expected next value, we simply apply the formula one more time assuming last value repeats
  expected_next_ema <- alpha * data[length(data)] + (1 - alpha) * ema[length(ema)]
  return(list(ema = ema, expected_next = expected_next_ema))
}



# Now, apply the calculate_ema_4th_order function to this lagged series
# Assuming you have initialized your dataframe and it's being updated elsewhere in your simulation

calculate_ema_4th_order_for_df <- function(df, column_name, n=4) {
  # Extract the column data based on column_name
  data <- df[[column_name]]
  
  # Filter out NA values that arise due to lagging
  data <- na.omit(data)
  
  # Apply the EMA calculation
  ema_result <- calculate_ema_4th_order(data, n)
  
  # Return the result
  return(ema_result)
}


```


```{r}
# Define the parameters for the Beta distribution based on the given mean and standard deviation
mean_beta <- 0.85
sd_beta <- 0.1
alpha_beta <- ((1 - mean_beta) / sd_beta^2 - 1 / mean_beta) * mean_beta^2
beta_beta <- alpha_beta * (1 / mean_beta - 1)


```


```{r}
library(readxl)
# Attempt to read the file without the leading directory path
averageweeklyproductionhours <- read_excel("averageweeklyproductionhours.xls")
employment=read_excel("Employment .xls")
gdpdeflator =read_excel("gdpdeflator.xlsx")
population =read_excel("population .xlsx")
investment =read_excel("fixedprivateinvestment .xls")
fedfunds=read_excel("FEDFUNDS.xls")
personalconsumption=read_excel("personalconsumption.xlsx")
realGDP=read_excel("realgdp.xlsx")
compensation =read_excel("compensation .xlsx")
GDPPOT <- read_excel("GDPPOT.xls")


```
```{r}
# List of your data frames
df_list <- list(averageweeklyproductionhours, employment, gdpdeflator, population, investment, fedfunds, personalconsumption, realGDP,compensation,GDPPOT)

# Function to convert the first column to Date and the second to numeric
convert_columns <- function(df) {
  df[[1]] <- as.Date(df[[1]], format = "%Y-%m-%d") # Adjust format as necessary
  df[[2]] <- as.numeric(df[[2]])
  return(df)
}

# Apply the function to each data frame in the list
converted_df_list <- lapply(df_list, convert_columns)

# If you want to replace the original data frames in your environment with the converted ones:
names(converted_df_list) <- c("averageweeklyproductionhours", "employment", "gdpdeflator", "population", "investment", "fedfunds", "personalconsumption", "realGDP","compensation","GDPPOT")
list2env(converted_df_list, envir = .GlobalEnv)

```
```{r}
# Assuming the converted data frames are stored in 'converted_df_list' and all have a common date column named 'Date'
# If the date column has different names across data frames, you'll need to rename them to be consistent first

# Example of renaming the first column to 'Date' for all data frames if needed
converted_df_list <- lapply(converted_df_list, function(df) {
  names(df)[1] <- 'Date'
  return(df)
})

# Merging all data frames on the 'Date' column
merged_df <- Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE), converted_df_list)

# The 'all = TRUE' argument ensures that all rows from each data frame are kept in the merged data frame, 
# even if there are missing matches in the 'Date' column from any of the data frames

# View the merged data frame
View(merged_df)

```
```{r}

# Assuming your data frame is named df
merged_df <- merged_df[387:722, ]


```

```{r}
# Assuming 'merged_df' is your merged data frame
cleaned_df <- na.omit(merged_df)

# View the cleaned data frame
View(cleaned_df)

```
```{r}
library(dplyr)
cleaned_df<- cleaned_df %>%
mutate(potential_gdp = GDPPOT* 1000000000)
```

```{r}

# library(lubridate)

final <- cleaned_df %>%
  mutate(
    Output_growth = log(`real gdp` / population) * 100,
    potential_gdp=log( potential_gdp/ population) * 100,
    Consumption_growth = log((PCE / `gdp deflator`) / population) * 100,
    Investment_growth = log((fpi / `gdp deflator`) / population) * 100,
    Wage_growth = log(compensation / `gdp deflator`) * 100,
    Hours = log((`weekly_production_hours` * (employment / 100) / population)) * 100,
    Inflation = log(`gdp deflator` / lag(`gdp deflator`, 1)) * 100,
    Ffr = FEDFUNDS / 4
  )
 

```

```{r}
# Real-time running average calculation
real_time_running_average <- function(data, i, window_size) {
    if (i < window_size) {
        # If less data than window size, calculate mean of available data
        return(mean(data[1:i]))
    } else {
        # Calculate running average of the window size ending at current index i
        return(mean(data[(i-window_size+1):i]))
    }
}
```

```{r}
final=final[2:112,]
```

```{r}
# Initialize the dataframe with the first 4rhows set to 1
my_data <- data.frame(
    time = 1:111,
   
    
    K_hat=rep(350,111),
    r_k_star=rep(-500,111),
    
   
    Q_hat = rep(14,111),
    pi_bar = rep(3,111),
    epsilon_C = rep(1, 111),
    epsilon_Q = rep(1, 111),
     epsilon_I = rep(1,111),
    epsilon_W = rep(1, 111),
    epsilon_R = rep(1, 111),
    epsilon_Pi=rep(1,111),
    epsilon_L=rep(1,111),
    epsilon_G=rep(1,111),
    
     epsilon_pitwo=rep(1,111),
    expected_K_hat_next=rep(350,111),
    my_data$expected_Pi_hat_next <- rep(1, 111),
my_data$expected_Q_hat_next <- rep(1, 111),
my_data$expected_R_hat_next <- rep(1, 111),
my_data$expected_W_hat_next <- rep(1, 111),
my_data$expected_L_hat_next <- rep(1, 111),

    
   # C_expect=rep(559.3016,111),
    #C_expect_lag=rep(559.3016,111),
    #I_expect = rep(437.489, 111),
  #I_expect_lag = rep(437.489, 111),
  #W_expect = rep(-44.7975, 111),
  #W_expect_lag = rep(-44.7975, 111),
  #H_expect = rep(-154.5826, 111),
  #H_expect_lag = rep(-154.5826, 111),
  #PI_expect = rep(2.851030, 111),
  #PI_expect_lag = rep(2.851030, 111),
  #Q_expect = rep(13.0000000, 111),
  #Q_expect_lag = rep(13.0000000, 111),
    
    expected_Q_hat_next = rep(14, 111),
   
  

   # Lagged values for the firstrhow would be NA
    Q_hat_lag1 = rep(14, 111),
    
    K_hat_lag1 = rep(350,111)
)
 my_data$C_hat =final$Consumption_growth
 my_data$C_hat_lag1=final$Consumption_growth

  my_data$I_hat_lag1 = final$Investment_growth
    my_data$expected_I_hat_next=final$Investment_growth
   my_data$I_hat = final$Investment_growth
   my_data$I_hat_lag1 = final$Investment_growth
    Y_hat=final$Output_growth
    Y_hat_lag1=final$Output_growth
     my_data$expected_Y_hat_next = final$Output_growth
     my_data$Pi_hat=final$Inflation
     my_data$Pi_hat_lag1=final$Inflation
     my_data$expected_Pi_hat_next=final$Inflation
     my_data$R_hat=final$FEDFUNDS
     my_data$R_hat_lag1=final$FEDFUNDS
     my_data$expected_R_hat_next=final$FEDFUNDS
     my_data$L_hat=final$Hours
     my_data$L_hat_lag1=final$Hours
     my_data$expected_L_hat_next=final$Hours
     my_data$W_hat=final$Wage_growth
     my_data$W_hat_lag1=final$Wage_growth
     my_data$expected_W_hat_next=final$Wage_growth
     my_data$potential_gdp=final$potential_gdp
     
```
```{r}
# Before the loop, call the calculate_ema_4th_order function to get expected values
for (i in 2:111) {
    # Update lagged values
    my_data$C_hat_lag1[i] <- my_data$C_hat[i-1]
    my_data$I_hat_lag1[i] <- my_data$I_hat[i-1]
    my_data$Q_hat_lag1[i] <- my_data$Q_hat[i-1] # Noting the case sensitivity
    my_data$C_expect_lag[i] = my_data$C_expect[i-1]
    my_data$Pi_hat_lag1[i] <- my_data$Pi_hat[i-1]
    my_data$K_hat_lag1[i] <- my_data$K_hat[i-1]
    my_data$W_hat_lag1[i] <- my_data$W_hat[i-1]
    
    my_data$Y_hat_lag1[i] <- my_data$Y_hat[i-1]
    my_data$YP_hat_lag1[i] <- my_data$YP_hat[i-1]
     my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
    my_data$L_hat_lag1[i] <- my_data$L_hat[i-1]
my_data$H_expect_lag[i] <- my_data$H_expect[i-1]
my_data$W_expect_lag[i] <- my_data$W_expect[i-1]
my_data$I_expect_lag[i] <- my_data$I_expect[i-1]
my_data$PI_expect_lag[i] <- my_data$PI_expect[i-1]
my_data$Q_expect_lag[i] <- my_data$Q_expect[i-1]
    
    
     # Generate new shock value for epsilon_c for the currentrhow
    eta_c <- rnorm(1, mean = 0, sd = sigma_eta_C)
    
# Simulate a persistence parameter from the Beta distribution
    rho_c<- rbeta(1, alpha_beta, beta_beta)
     my_data$epsilon_C[i] <- rho_c * my_data$epsilon_C[i-1] + eta_c
       # Generate new shock value for epsilon_q for the current row
     epsilon_Q<- rnorm(1, mean = 0, sd = sigma_e_q)
       my_data$epsilon_Q[i] <- epsilon_Q
       eta_I<- rnorm(1, mean = 0, sd = sigma_e_i)
       rho_q<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_I[i] <- rho_q * my_data$epsilon_I[i-1] + eta_I
      
       epsilon_W <- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
      epsilon_p <- rnorm(1, mean = 0, sd = sigma_e_p)
      my_data$epsilon_Pi[i]=epsilon_p
      
        rho_pi<- rbeta(1, alpha_beta, beta_beta)
        eta_pitwo<- rnorm(1, mean = 0, sd = sigma_pi_bar)
       my_data$epsilon_pitwo[i] <- rho_pi * my_data$epsilon_pitwo[i-1] + eta_pitwo
        eta_g<- rnorm(1, mean = 0, sd = sigma_e_q)
        rho_g<- rbeta(1, alpha_beta, beta_beta)
       my_data$epsilon_G[i] <- rho_g * my_data$epsilon_G[i-1] + eta_g
       epsilon_W<- rnorm(1, mean = 0, sd = sigma_e_w)
       my_data$epsilon_W[i]=epsilon_W
       epsilon_R<- rnorm(1, mean = 0, sd = sigma_er)
       my_data$epsilon_R[i]=epsilon_R
     # Update pi_bar using the running average of Pi_hat
    my_data$pi_bar[i] <- real_time_running_average(my_data$Pi_hat, i, window_size = 5)  # window size is an example, adjust as needed
 part1 <- (h / (1 + h)) * my_data$C_hat_lag1[i]
part2 <- (1 / (1 + h)) * my_data$expected_C_hat_next[i]
part3 <- ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * ( my_data$expected_L_hat_next[i]- my_data$L_hat[i])
part4 <- -((1 - h) / ((1 + h)) * sigma_c) * (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i])
part5 <- ((1 - h) / ((1 + h) * sigma_c)) * my_data$epsilon_C[i]
# Combine all parts to calculate C_hat for the current index

# Isolate expected_C_hat_next[i]
my_data$expected_C_hat_next[i] <- (my_data$C_hat[i] - part1 - part3 - part4 - part5) * (1 + h)


        
  # Output gap equation
my_data$Q_hat[i] = -(my_data$R_hat[i] - my_data$expected_Pi_hat_next[i]) +
       ((1-tao) / (1 - tao + r_k_star)) * my_data$expected_Q_hat_next[i] +
        (r_k_star / (1 - tao + r_k_star)) * my_data$expected_R_hat_next[i] + my_data$epsilon_Q[i]
my_data$expected_Q_hat_next[i] <- (my_data$Q_hat[i] + (my_data$R_hat[i] - my_data$expected_Pi_hat_next[i]) - (r_k_star / (1 - tao + r_k_star)) * my_data$expected_R_hat_next[i] - my_data$epsilon_Q[i]) / ((1-tao) / (1 - tao + r_k_star))

my_data$I_hat[i] = (1 / (1 + Beta)) * my_data$I_hat_lag1[i] +   
   (Beta / (1 + Beta)) * my_data$expected_I_hat_next[i] +
         ((1/phi_uppercase)/(1+Beta))*my_data$Q_hat[i]+my_data$epsilon_I[i]
 
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
    
   my_data$L_hat[i] = -my_data$W_hat[i] + my_data$R_hat[i]+ my_data$K_hat_lag1[i]
   
   my_data$Y_hat[i] = (1 - (tao*ky) - gy) * my_data$C_hat[i] + (tao*ky) * my_data$I_hat[i] + gy *my_data$epsilon_G[i]
   # Break down the equation into parts as before
    part1w <- (Beta / (1 + Beta)) * my_data$expected_W_hat_next[i-1]
    part2w <- (1 / (1 + Beta)) * my_data$W_hat_lag1[i]
    part3w <- (Beta / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    part4w <- ((1 + (Beta * gamma_w)) / (1 + Beta)) * (my_data$Pi_hat_lag1[i] - my_data$pi_bar[i])
    part5w <- (gamma_w / (1 + Beta)) * (my_data$expected_Pi_hat_next[i-1] - my_data$pi_bar[i])
    # Calculate the complex adjustment part
    adjustment_factor <- (1/(1+Beta))*((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws)
    complex_part <- adjustment_factor *( (my_data$W_hat_lag1[i] - (sigma_L * my_data$L_hat_lag1[i]) - (1 / (1 - h) )* (my_data$C_hat[i] - my_data$C_hat_lag1[i])  ))+my_data$epsilon_L[i] 
    # Combine all parts for the final calculation of W_hat
   my_data$W_hat[i] <- part1w + part2w + part3w - part4w + part5w - complex_part + my_data$epsilon_W[i]
      
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
  my_data$r_k_star[i]=-(my_data$K_hat[i]- my_data$L_hat[i])+my_data$W_hat[i]
    
    }
```


```{r}
C_random<- rnorm(1, mean = 0, sd = 1)
 # Generate random normal variables for each letter
alpha_C_random <- rnorm(1, mean = 0, sd = 1)
alpha_I_random <- rnorm(1, mean = 0, sd = 1)
alpha_W_random <- rnorm(1, mean = 0, sd = 1)
alpha_H_random <- rnorm(1, mean = 0, sd = 1)
alpha_PI_random <- rnorm(1, mean = 0, sd = 1)
alpha_Q_random <- rnorm(1, mean = 0, sd = 1)
# Update C_expect
my_data$C_expect[i] <- my_data$C_hat[i] + 0.8 * (my_data$C_hat_lag1[i] - my_data$C_expect_lag[i]) + alpha_C_random

# Update I_expect
my_data$I_expect[i] <- my_data$I_hat[i] + 0.8 * (my_data$I_hat_lag1[i] - my_data$I_expect_lag[i]) + alpha_I_random

# Update W_expect
my_data$W_expect[i] <- my_data$W_hat[i] + 0.8 * (my_data$W_hat_lag1[i] - my_data$W_expect_lag[i]) + alpha_W_random

# Update H_expect
my_data$H_expect[i] <- my_data$L_hat[i] + 0.8 * (my_data$L_hat_lag1[i] - my_data$H_expect_lag[i]) + alpha_H_random

# Update PI_expect
my_data$PI_expect[i] <- my_data$Pi_hat[i] + 0.8 * (my_data$Pi_hat_lag1[i] - my_data$Pi_hat_lag1[i]) + alpha_PI_random

# Update Q_expect
my_data$Q_expect[i] <- my_data$Q_hat[i] + 0.8 * (my_data$Q_hat_lag1[i] - my_data$Q_expect_lag[i]) + alpha_Q_random

   # Update pi_bar using the running average of Pi_hat
    my_data$pi_bar[i] <- real_time_running_average(my_data$Pi_hat, i, window_size = 5)  # window size is an example, adjust as needed
 
 my_data$K_hat[i]=(1-tao)*my_data$K_hat_lag1[i]+tao*my_data$I_hat_lag1[i]
   tao*my_data$epsilon_I[i]
  my_data$r_k_star[i]=-(my_data$K_hat[i]- my_data$L_hat[i])+my_data$W_hat[i]
    
    }

  
```

```{r}
library(dplyr)

# Assuming 'final' is your data frame
my_data <- my_data %>% mutate(inflationtargot = 0.35)
```


```{r}
library(rstan)
```


```{r}
# Calculate 'T' as the number of observations
T <- nrow(final)  # This should be a single integer, not a vector

# Combine vectors into a new dataframe with 'T' as a separate element
data_list <- list(
  T = T,  # 'T' is a single integer, not a vector
  epsilon_C = my_data$epsilon_C,
  epsilon_Q = my_data$epsilon_Q,
  epsilon_I = my_data$epsilon_I,
  epsilon_PI = my_data$epsilon_Pi,
  epsilon_G = my_data$epsilon_G,
  epsilon_R = my_data$epsilon_R,
  r_k_star=my_data$r_k_star,
  epsilon_W = my_data$epsilon_W,
  output_growth_vector = final$Output_growth,
  consumption_growth_vector = final$Consumption_growth,
  investment_growth_vector = final$Investment_growth,
  wage_growth_vector = final$Wage_growth,
  hours_vector = final$Hours,
  inflation_vector = final$Inflation,
  ffr_vector = final$Ffr,
  kapital = my_data$K_hat,
  q = my_data$Q_hat,
 
  pi_bar=my_data$pi_bar,
  epsilon_pitwo= my_data$epsilon_pitwo,
  epsilon_L= my_data$epsilon_L
)

# Ensure the list is correctly structured for Stan
if(!all(sapply(data_list[-1], length) == T)) {  # Exclude 'T' from the length check
  stop("The lengths of the vectors do not match the value of T.")
}



```
```{r}

# Check lengths of the variables
lengths <- sapply(list(
  my_data$epsilon_C,
  my_data$potential_gdp,
  my_data$epsilon_Q,
  my_data$epsilon_I,
  my_data$epsilon_Pi,
  my_data$epsilon_G,
  my_data$epsilon_R,
  my_data$r_k_star,
  my_data$epsilon_W,
  my_data$inflationtargot,
  final$Output_growth,
  final$Consumption_growth,
  final$Investment_growth,
  final$Wage_growth,
  final$Hours,
  final$Inflation,
  final$Ffr,
  my_data$K_hat,
  my_data$Q_hat,
  my_data$pi_bar,
  my_data$epsilon_pitwo,
  my_data$epsilon_L
), length)

# Print lengths to check if all are at least T
print(lengths)

# Ensure that all required vectors are at least length T
if (any(lengths < T)) {
  stop("One or more vectors are shorter than T")
}

# Combine vectors into a new list for Stan
data_list <- list(
  T = T,  # 'T' is a single integer, not a vector
  epsilon_C = my_data$epsilon_C[1:T],
  epsilon_Q = my_data$epsilon_Q[1:T],
  epsilon_I = my_data$epsilon_I[1:T],
  epsilon_PI = my_data$epsilon_Pi[1:T],
  epsilon_G = my_data$epsilon_G[1:T],
  epsilon_R = my_data$epsilon_R[1:T],
  r_k_star = my_data$r_k_star[1:T],
  inflationtaget=my_data$inflationtargot,
  epsilon_W = my_data$epsilon_W[1:T],
  potential_gdp =my_data$potential_gdp[1:T],
  output_growth_vector = final$Output_growth[1:T],
  consumption_growth_vector = final$Consumption_growth[1:T],
  investment_growth_vector = final$Investment_growth[1:T],
  wage_growth_vector = final$Wage_growth[1:T],
  hours_vector = final$Hours[1:T],
  inflation_vector = final$Inflation[1:T],
  ffr_vector = final$Ffr[1:T],
  kapital = my_data$K_hat[1:T],
  q = my_data$Q_hat[1:T],
  pi_bar = my_data$pi_bar[1:T],
  epsilon_pitwo = my_data$epsilon_pitwo[1:T],
  epsilon_L = my_data$epsilon_L[1:T],
  shock_time = 50,  # Example time point
  shock_value = -0.3  # Example shock value
)



```

```{r}
library(V8)
```

```{stan output.var="model"}
data {
  int<lower=1> T; // Number of time points
  vector[T] epsilon_C;
  vector[T] epsilon_Q;
  vector[T] epsilon_I;
  vector[T] epsilon_PI;
   vector[T] epsilon_G;
   vector[T] epsilon_R;
   vector[T] epsilon_W;
  vector[T] consumption_growth_vector ;    // Observed data
  vector[T] output_growth_vector;
  vector[T]  potential_gdp;
  vector[T] investment_growth_vector;
  vector[T] wage_growth_vector;
  vector[T] hours_vector;
  vector[T] inflation_vector;
  vector[T] ffr_vector;
  vector[T] kapital;
  vector[T] q;
  vector[T]  r_k_star;
  vector[T]   inflationtaget;
  vector[T] epsilon_L;
  vector[T] epsilon_pitwo;
   int<lower=1, upper=T> shock_time; // Time of the shock
  real shock_value; // Magnitude of the shock
    // Standard deviation for eta_c
} 

parameters {
 
    real<lower=0.95, upper=1> Beta; // Given mean is 0.025, the range would be truncated because Beta cannot be negative
  real<lower=0.019, upper=0.035> tao;
  real<lower=0, upper=0.8> alpha; // Mean=0.3, so range is 0.3±0.5, lower bound truncated at 0
  real<lower=0, upper=1> psi; // Mean=1/.169, requires calculating the actual value and then ±0.5, truncating bounds as necessary
  real<lower=0.3, upper=0.5> gamma_pi; // Mean=0.469, so range is 0.469±0.5, upper bound truncated at 1
  real<lower=0.5, upper=1> gamma_w; // Mean=0.7111, so range is 0.7111±0.5, upper bound truncated at 1
   
  real<lower=0.7, upper=2> alpha_C;  // Consumption growth scaling parameter
  
  real<lower=0.9, upper=1> alpha_I;  // Investment growth scaling parameter
  real<lower=0.9, upper=1.7> alpha_W;  // Wage growth scaling parameter
  real<lower=0.5, upper=2.3> alpha_H;  // Hours scaling parameter
 
    
  real<lower=0.3, upper=0.7> lambda_w;
  real<lower=0.9, upper=1> zeta_ps;
  real<lower=0.5, upper=1> zeta_ws;
  real<lower=1.9, upper=2.7> sigma_L;
  real<lower=0.853, upper=1.853> sigma_c;
  real<lower=0.1, upper=0.6> h;
  real<lower=0.908, upper=1.908> phi;
  real<lower=0.9, upper=1.25> phi_uppercase; // Mean=1/6.771, requires calculating the actual value and then ±0.5, truncating bounds as necessary
  real<lower=0, upper=1> rbar_k; // Depends on Beta, the range might need to be dynamically calculated
  real<lower=8.3, upper=9.3> Ky;
  real<lower=0, upper=0.1116> sigma_eta_C;
  real<lower=0.15, upper=0.25> Invy;
  real<lower=0.3, upper=0.9> Cy; // Mean=0.6, so range is 0.6±0.5, upper bound truncated at 1
 
  real<lower=0, upper=1> gy; // Computed from 1-Cy-Invy, so range needs to be calculated based on those values
  real<lower=0.3, upper=0.84> r_pi;
  real<lower=0, upper=0.599> r_y;
  real<lower=0, upper=0.659> Ry_delta;
   real<lower=0.8, upper=1> rho;
  real<lower=0.7, upper=1> rho_el;
  real<lower=0.7, upper=1> rho_ea;
  real<lower=0.7, upper=1> rho_eb;
  real<lower=0.7, upper=1> rho_G;
  real<lower=0.7, upper=1> rho_pi;
  real<lower=0.7, upper=1> rh_i;
  
  
 // real<lower=0> rho_er; // Since rho_er=0, it is set as the lower bound
  real<lower=0, upper=1> rho_lamdaw; // Since rho_lamdaw=0, it is set as the lower bound
  real<lower=0, upper=1> rho_q; // Since rho_q=0, it is set as the lower bound
  real<lower=0, upper=1> rho_lamda_p; // Since rho_lamda_p=0, it is set as the lower bound
  
  // r_k_star is computed from Beta and tao, and will need to have a dynamic range based on those values.
 
  // rho_L has a given value of 0.9, the range would be 0.4 to 1 since it can't exceed 1
  real<lower=0.4, upper=1> rho_L;
  
}
 transformed parameters {
  vector[T] output_growth_shocked;
  vector[T] consumption_growth_shocked;
  vector[T] investment_growth_shocked;
  vector[T] wage_growth_shocked;
  vector[T] hours_shocked;
  vector[T] potential_gdp_shocked;
  vector[T] inflation_shocked;
  vector[T] ffr_shocked;
  vector[T] kapital_shocked;
  vector[T] q_shocked;
  vector[T] r_k_star_shocked;
  vector[T]  inflationtaget_shocked;
  output_growth_shocked=output_growth_vector;
  consumption_growth_shocked=consumption_growth_vector;
  investment_growth_shocked=investment_growth_vector;
   wage_growth_shocked=wage_growth_vector;
    hours_shocked= hours_vector;
    inflation_shocked=inflation_vector;
   ffr_shocked = ffr_vector;
  kapital_shocked = kapital;
  q_shocked = q;
  r_k_star_shocked = r_k_star;
  inflation_shocked = inflation_vector;
  inflationtaget_shocked = inflationtaget;
  potential_gdp_shocked = potential_gdp;
  output_growth_shocked = output_growth_vector;

  // Apply a 30% decrease in output growth at the specified time
 for (t in shock_time:min(shock_time + 4, T)) {
  inflation_shocked[t] = inflation_shocked[t] + 1;
}


  // Propagate the shock through other series
  for (t in (shock_time + 1):T) {
        ffr_shocked[t] = inflation_shocked[t-1] + 
                     0.5 * ((output_growth_shocked[t-1] - potential_gdp_shocked[t-1])/ potential_gdp_shocked[t-1]) + 
                     0.5 * (inflation_shocked[t-1] -inflationtaget_shocked[t-1]) +inflationtaget_shocked[t-1] +epsilon_R[t];
    real part1 = (h / (1 + h)) * consumption_growth_shocked[t-1]; 
    real part2 = (1 / (1 + h)) * consumption_growth_shocked[t]*alpha_C; 
    real part3 = ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * (hours_shocked[t-1] - hours_shocked[t]*alpha_H);
    real part4 = -((1 - h) / ((1 + h)) * sigma_c) * (ffr_shocked[t] - inflation_shocked[t-1]);
    real part5 = ((1 - h) / ((1 + h) * sigma_c)) * epsilon_C[t];

    consumption_growth_shocked[t] = part1 + part2 + part3 + part4 + part5;

    q_shocked[t] = -(ffr_shocked[t] - inflation_shocked[t-1]) + ((1-tao) / (1 - tao + r_k_star_shocked[t])) * q_shocked[t-1] + (r_k_star_shocked[t] / (1 - tao + r_k_star_shocked[t])) * ffr_shocked[t-1] + epsilon_Q[t];
    
    investment_growth_shocked[t] = (1 / (1 + Beta)) * investment_growth_shocked[t-1] + (Beta / (1 + Beta)) * investment_growth_shocked[t]*alpha_I + ((1/phi_uppercase)/(1+Beta)) * q_shocked[t] + epsilon_I[t];
    
    

    kapital_shocked[t] = (1 - tao) * kapital_shocked[t - 1] + tao * investment_growth_shocked[t] + tao * epsilon_I[t];

    hours_shocked[t] = -wage_growth_shocked[t] + (1 + psi) * ffr_shocked[t] + kapital_shocked[t-1];

   output_growth_shocked[t] = (1 - (tao*Ky) - gy) * consumption_growth_shocked[t] + (tao*Ky) * investment_growth_shocked[t] + gy * epsilon_G[t];

    real parta = (Beta / (1 + Beta * gamma_pi)) * (inflation_shocked[t-1] - inflationtaget_shocked[t]);
    real partb = (gamma_pi / (1 + Beta * gamma_pi)) * (inflation_shocked[t-1] - inflationtaget_shocked[t]);
    real partc = (1/(1 - Beta * gamma_pi)) * (((1 - Beta * zeta_ps) * (1 - zeta_ps)) / zeta_ps) * (alpha * ffr_shocked[t] + ((1 - alpha) * wage_growth_shocked[t]) - epsilon_pitwo[t] - (1 - alpha) * gamma_w);
    real parte = epsilon_PI[t];
   inflation_shocked[t] = parta + partb + partc + parte +  inflationtaget_shocked[t];

    real part1w = (Beta / (1 + Beta)) * wage_growth_shocked[t]*alpha_W;
    real part2w = (1 / (1 + Beta)) * wage_growth_shocked[t-1];
    real part3w = (Beta / (1 + Beta)) * (inflationtaget_shocked[t-1] - inflation_shocked[t]);
    real part4w = ((1 + (Beta * gamma_w)) / (1 + Beta)) * (inflationtaget_shocked[t-1] - inflation_shocked[t]);
    real part5w = (gamma_w / (1 + Beta)) * (inflationtaget_shocked[t-1] - inflation_shocked[t]);
    real adjustment_factor = (1 / (1 + Beta)) * ((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws);
    real complex_part = adjustment_factor * (wage_growth_shocked[t-1] - (sigma_L * hours_shocked[t]) - (1 / (1 - h)) * (consumption_growth_shocked[t] - (h * consumption_growth_shocked[t-1]))) + epsilon_L[t];
    wage_growth_shocked[t] = part1w + part2w + part3w - part4w + part5w - complex_part + epsilon_W[t];
  }
}


model {
  
  // Priors for parameters based on the provided table
  phi ~ normal(4.00, 1.50);
  //sigma_c ~ normal(1.50, 0.37);
  h ~ beta(0.3, 14);  // Beta parameters calculated from mean and standard deviation
  zeta_ws ~ beta(0.3, 12);  // Mean=0.5, SD=0.10 implies alpha=beta
  //sigma_e_L ~ normal(2.00, 0.75);
  zeta_ps ~ beta(0.3, 12);  // Mean=0.5, SD=0.10 implies alpha=beta
  lambda_w ~ beta(0.3, 5.055);  // Mean=0.5, SD=0.15 implies alpha=beta
  psi ~ beta(0.3, 5.05);  // Mean=0.5, SD=0.15 implies alpha=beta
  phi_uppercase ~ normal(1.25, 0.12);
  r_pi ~ normal(1.50, 0.25);
  
  r_y ~ normal(0.12, 0.05);
  Ry_delta ~ normal(0.12, 0.05);
  
  rbar_k ~ normal(0.40, 0.10);
  alpha ~ normal(0.30, 0.05);
   


// Normal distributions
alpha_C ~ normal(0.9855119, 0.023170470);
alpha_I ~ normal(0.9797158, 0.007438688);
alpha_W ~ normal(1.3491620, 0.088049984);
alpha_H ~ normal(1.3428200, 0.369923981);


   rho ~ beta(0.3, 2.625);
  rho_el ~ beta(0.3, 2.625);
  rho_ea ~ beta(0.3, 2.625);
  rho_eb ~ beta(0.3, 2.625);
  rho_G ~ beta(0.3, 2.625);
  rho_pi ~ beta(0.3, 2.625);
  rh_i ~ beta(0.3, 2.625);
  rho_L ~ beta(0.3, 2.625);
  // Likelihood
  

  for (t in 2:T)  {
    ffr_vector[t] ~ normal(inflation_vector[t-1] + 
                   (0.5 * (output_growth_vector[t-1] -potential_gdp[t-1])/potential_gdp[t-1]) + 
                   (0.5 * (inflation_vector[t-1] -  inflationtaget[t-1])) + 
                    inflationtaget[t] + epsilon_R[t],0.9217559);
    real part1 = (h / (1 + h)) * consumption_growth_vector[t-1]; 
    real part2 = (1 / (1 + h)) * consumption_growth_vector[t]*alpha_C; 
    real part3 = ((sigma_c - 1) / (sigma_c * (1 + lambda_w) * (1 + h))) * (hours_vector[t] - hours_vector[t]*alpha_H);
    real part4 = -((1 - h) / ((1 + h)) * sigma_c) * (ffr_vector[t] - inflation_vector[t]);
    real part5 = ((1 - h) / ((1 + h) * sigma_c)) * epsilon_C[t] ; // Assuming rho_c and eta_c are arrays generated in 'generated quantities'

    consumption_growth_vector[t] ~ normal(part1 + part2 + part3 + part4 + part5,18.22393);
     
     //r_k_star~ (-(kapital[t]-hours_vector[t])-wage_growth_vector[t],)
     //Output gap equation
 q[t] ~ normal(-(ffr_vector[t] - inflation_vector[t]) +
      ((1-tao) / (1 - tao + ffr_vector[t])) *q[t]+
    (r_k_star[t] / (1 - tao + ffr_shocked[t])) * ffr_vector[t-1]+epsilon_Q[t],5.701086);
    
    investment_growth_vector[t] ~ normal((1 / (1 + Beta)) * investment_growth_vector[t-1]     +(Beta / (1 + Beta)) * investment_growth_vector[t]*alpha_I +
         ((1/phi_uppercase)/(1+Beta))*q[t]+epsilon_I[t],17.244);
      
         
   
   kapital[t] ~ normal((1 - tao) * kapital[t - 1] + tao * investment_growth_vector[t] + tao *epsilon_I[t], 27.24975);

    
   hours_vector[t]~ normal(-wage_growth_vector[t] + (1+psi)*ffr_vector[t]+ kapital[t-1],2.57808);
   
   output_growth_vector[t] ~ normal((1 - (tao*Ky) - gy) * consumption_growth_vector[t] + (tao*Ky) * investment_growth_vector[t] + gy *epsilon_G[t],15.30617);
      
// Breaking down the calculation into parts
 real parta = (Beta / (1 + Beta * gamma_pi)) * (inflation_vector[t]-inflationtaget[t]);
 real partb = (gamma_pi / (1 + Beta * gamma_pi)) * (inflation_vector[t-1] -inflationtaget[t]) ;
 real partc =  (1/(1 - Beta * gamma_pi))* (((1-Beta*zeta_ps)*(1-zeta_ps))/zeta_ps)*(ffr_vector[t]+((1-alpha)*wage_growth_vector[t])-epsilon_pitwo[t]-(1-alpha)*gamma_w);
 real parte =epsilon_PI[t];
// Combine parts
  inflation_vector[t]~  normal(parta + partb + partc + parte +  inflationtaget[t],0.11154431);
   
     real part1w = (Beta / (1 + Beta)) * wage_growth_vector[t]*alpha_W;
    real part2w = (1 / (1 + Beta)) * wage_growth_vector[t-1];
    real part3w = (Beta / (1 + Beta)) * (inflationtaget[t-1] - inflation_vector[t]);
    real part4w = ((1 + (Beta * gamma_w)) / (1 + Beta)) * (inflationtaget[t-1] - inflation_vector[t]);
    real part5w = (gamma_w / (1 + Beta)) * (inflationtaget[t-1] -inflation_vector[t]);
    real adjustment_factor = (1/(1+Beta))*((1 - (Beta * zeta_ws) * (1 - zeta_ws)) / (1 + ((1 - lambda_w) * sigma_L) / lambda_w) * zeta_ws);
    real complex_part = adjustment_factor * (wage_growth_vector[t-1] - (sigma_L * hours_vector[t]) - (1 / (1 - h)) * (consumption_growth_vector[t] - (h * consumption_growth_vector[t-1]))) + epsilon_L[t]; 

    wage_growth_vector[t] ~ normal(part1w + part2w + part3w - part4w + part5w - complex_part + epsilon_W[t], 11.03177);
   }
 }
 
generated quantities {
  vector[T] output_growth_shocked_output;
  vector[T] consumption_growth_shocked_output;
  vector[T] investment_growth_shocked_output;
  vector[T] wage_growth_shocked_output;
  vector[T] hours_shocked_output;
  vector[T] inflation_shocked_output;
  vector[T] ffr_shocked_output;
  

  output_growth_shocked_output = output_growth_shocked;
  consumption_growth_shocked_output = consumption_growth_shocked;
  investment_growth_shocked_output = investment_growth_shocked;
  wage_growth_shocked_output = wage_growth_shocked;
  hours_shocked_output = hours_shocked;
  inflation_shocked_output = inflation_shocked;
  ffr_shocked_output = ffr_shocked;
   
}







```



```{r}
fit <- sampling(model, data =data_list , chains = 2, iter = 6000, warmup = 1000)

```
```{r}
library(fitdistrplus)
library(dplyr)


# Extract samples from the fitted model
samples <- rstan::extract(fit)

# Variables of interest
variables <- c("alpha_C", "alpha_I", "alpha_W", "alpha_H", "alpha_PI", "alpha_Q")

# Organize samples into a list
samples_list <- lapply(variables, function(var) {
  as.vector(samples[[var]])
})
names(samples_list) <- variables

# Function to fit normal and beta distributions and return the results
fit_distributions <- function(var, samples) {
  result <- try({
    # Fit normal distribution
    normal_fit <- fitdist(samples, "norm")
    normal_params <- normal_fit$estimate
    # Check for non-finite values in normal_params
    if (any(!is.finite(normal_params))) {
      message(paste("Skipping normal distribution fitting for", var, "due to non-finite values"))
      return(NULL)
    }
    
    # Scale samples to (0, 1) for beta distribution fitting
    scaled_samples <- (samples - min(samples)) / (max(samples) - min(samples))
    # Ensure no values are exactly 0 or 1
    scaled_samples[scaled_samples == 0] <- 0.0001
    scaled_samples[scaled_samples == 1] <- 0.9999

    # Fit beta distribution
    beta_fit <- fitdist(scaled_samples, "beta", start = list(shape1 = 1, shape2 = 7.6))
    beta_params <- beta_fit$estimate
    # Check for non-finite values in beta_params
    if (any(!is.finite(beta_params))) {
      message(paste("Skipping beta distribution fitting for", var, "due to non-finite values"))
      return(NULL)
    }
    
    return(list(normal = normal_params, beta = beta_params))
  }, silent = TRUE)
  
  if (inherits(result, "try-error")) {
    message(paste("Fitting failed for", var))
    return(NULL)
  }
  
  return(result)
}

# Initialize an empty data frame to store the results
results_df <- data.frame(
  variable = character(),
  param1 = numeric(),
  param2 = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each variable and fit the distributions
for (var in names(samples_list)) {
  fit_result <- fit_distributions(var, samples_list[[var]])
  if (!is.null(fit_result)) {
    if (!is.null(fit_result$normal)) {
      results_df <- results_df %>%
        bind_rows(data.frame(
          variable = var,
          param1 = fit_result$normal[1],
          param2 = fit_result$normal[2],
          stringsAsFactors = FALSE
        ))
    }
    if (!is.null(fit_result$beta)) {
      results_df <- results_df %>%
        bind_rows(data.frame(
          variable = paste0(var, "_beta"),
          param1 = fit_result$beta[1],
          param2 = fit_result$beta[2],
          stringsAsFactors = FALSE
        ))
    }
  }
}

# Print the results
print(results_df)

    


```
```{r}
library(ggplot2)


# Extract samples from the fitted model
samples <- rstan::extract(fit)

# Variables of interest
variables <- c("alpha_C", "alpha_I", "alpha_W", "alpha_H")

# Organize samples into a list and convert to data frame for ggplot2
samples_list <- lapply(variables, function(var) {
  data.frame(value = as.vector(samples[[var]]), variable = var)
})

# Combine all samples into a single data frame
df <- do.call(rbind, samples_list)

# Plot density plots for all variables
plots <- list()
for (var in variables) {
  p <- ggplot(df[df$variable == var, ], aes(x = value)) +
    geom_density(fill = "blue", alpha = 0.5) +
    labs(title = paste("Density Plot of", var), x = var, y = "Density") +
    theme_minimal()
  
  # Save the plot to a list
  plots[[var]] <- p
  
  # Optionally, save each plot as an image file
  ggsave(filename = paste0(var, "_density_plot.png"), plot = p, width = 6, height = 4)
}

# Display the plots
for (var in variables) {
  print(plots[[var]])
}


```


```{r}
# Extract samples from the fitted model
samples <- rstan::extract(fit)

# Check if the variables exist and have correct dimensions
print(names(samples))
print(dim(samples$output_growth_shocked))
print(dim(samples$consumption_growth_shocked))
print(dim(samples$investment_growth_shocked))
print(dim(samples$wage_growth_shocked))
print(dim(samples$hours_shocked))
print(dim(samples$inflation_shocked))
print(dim(samples$ffr_shocked))


```
```{r}
# Calculate the mean and credible intervals for each shocked variable

# Output Growth
mean_output_growth_shocked <- apply(samples$output_growth_shocked_output, 2, mean)
lower_credible_interval_output <- apply(samples$output_growth_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_output <- apply(samples$output_growth_shocked_output, 2, quantile, probs = 0.975)

# Consumption Growth
mean_consumption_growth_shocked <- apply(samples$consumption_growth_shocked_output, 2, mean)
lower_credible_interval_consumption <- apply(samples$consumption_growth_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_consumption <- apply(samples$consumption_growth_shocked_output, 2, quantile, probs = 0.975)

# Investment Growth
mean_investment_growth_shocked <- apply(samples$investment_growth_shocked_output, 2, mean)
lower_credible_interval_investment <- apply(samples$investment_growth_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_investment <- apply(samples$investment_growth_shocked_output, 2, quantile, probs = 0.975)

# Wage Growth
mean_wage_growth_shocked <- apply(samples$wage_growth_shocked_output, 2, mean)
lower_credible_interval_wage <- apply(samples$wage_growth_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_wage <- apply(samples$wage_growth_shocked_output, 2, quantile, probs = 0.975)
# potential_gdp
potential_gdp<- apply(samples$potential_gdp_shocked, 2, mean)
lower_credible_interval_hours <- apply(samples$potential_gdp_shocked, 2, quantile, probs = 0.025)
upper_credible_interval_hours <- apply(samples$potential_gdp_shocked, 2, quantile, probs = 0.975)
# Hours
mean_hours_shocked <- apply(samples$hours_shocked_output, 2, mean)
lower_credible_interval_hours <- apply(samples$hours_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_hours <- apply(samples$hours_shocked_output, 2, quantile, probs = 0.975)

# Inflation
mean_inflation_shocked <- apply(samples$inflation_shocked_output, 2, mean)
lower_credible_interval_inflation <- apply(samples$inflation_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_inflation <- apply(samples$inflation_shocked_output, 2, quantile, probs = 0.975)

# Federal Funds Rate
mean_ffr_shocked <- apply(samples$ffr_shocked_output, 2, mean)
lower_credible_interval_ffr <- apply(samples$ffr_shocked_output, 2, quantile, probs = 0.025)
upper_credible_interval_ffr <- apply(samples$ffr_shocked_output, 2, quantile, probs = 0.975)





```


```{r}
library(ggplot2)
library(gridExtra)

# Create a function to plot a variable with its credible intervals
plot_shocked_variable <- function(original, mean_shocked, lower_ci, upper_ci, variable_name) {
  data <- data.frame(
    Time = 1:length(original),
    Original = original,
    Mean_Shocked = mean_shocked,
    Lower_CI = lower_ci,
    Upper_C_I = upper_ci
  )
  
  ggplot(data, aes(x = Time)) +
    geom_line(aes(y = Original), color = "blue", linetype = "dashed") +
    geom_line(aes(y = Mean_Shocked), color = "red") +
    geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_C_I), fill = "red", alpha = 0.2) +
    labs(title = paste("Original and Shocked", variable_name, "Series"), x = "Time", y = variable_name) +
    theme_minimal() +
    theme(
      plot.title = element_text(size = 10),
      axis.title.x = element_text(size = 8),
      axis.title.y = element_text(size = 8),
      axis.text = element_text(size = 6),
      legend.title = element_text(size = 8),
      legend.text = element_text(size = 6)
    )
}

# Plot each variable
plot_list <- list(
  plot_shocked_variable(data_list$output_growth_vector, mean_output_growth_shocked, lower_credible_interval_output, upper_credible_interval_output, "Output Growth"),
  plot_shocked_variable(data_list$consumption_growth_vector, mean_consumption_growth_shocked, lower_credible_interval_consumption, upper_credible_interval_consumption, "Consumption Growth"),
  plot_shocked_variable(data_list$investment_growth_vector, mean_investment_growth_shocked, lower_credible_interval_investment, upper_credible_interval_investment, "Investment Growth"),
  plot_shocked_variable(data_list$wage_growth_vector, mean_wage_growth_shocked, lower_credible_interval_wage, upper_credible_interval_wage, "Wage Growth"),
  plot_shocked_variable(data_list$hours_vector, mean_hours_shocked, lower_credible_interval_hours, upper_credible_interval_hours, "Hours"),
  plot_shocked_variable(data_list$inflation_vector, mean_inflation_shocked, lower_credible_interval_inflation, upper_credible_interval_inflation, "Inflation"),
  plot_shocked_variable(data_list$ffr_vector, mean_ffr_shocked, lower_credible_interval_ffr, upper_credible_interval_ffr, "Federal Funds Rate")
)

# Arrange the plots in a grid
grid.arrange(grobs = plot_list, ncol = 2)

```
```{r}


# Extract parameters
params <- extract(fit)


# List of variable names corresponding to the state transition matrix
variable_names <- c(
  "Beta", "tao", "alpha", "psi", "gamma_pi", "gamma_w",
  "lambda_w", "zeta_ps", "zeta_ws", "sigma_L",
  "sigma_c", "h", "phi", "phi_uppercase", "rbar_k", "Ky",
  "sigma_eta_C", "Invy", "Cy", "gy",
  "r_pi", "r_y", "Ry_delta", "rho",
  "rho_el", "rho_ea", "rho_eb", "rho_G", "rho_pi", "rh_i",
  "rho_lamdaw", "rho_q", "rho_lamda_p", "rho_L", "output_growth_shocked", "consumption_growth_shocked",
  "investment_growth_shocked", "wage_growth_shocked", "hours_shocked", "potential_gdp_shocked",
  "inflation_shocked", "ffr_shocked", "kapital_shocked", "q_shocked", "r_k_star_shocked", "inflationtaget_shocked",
  "output_growth_shocked_output", "consumption_growth_shocked_output", "investment_growth_shocked_output", "wage_growth_shocked_output",
  "hours_shocked_output", "inflation_shocked_output", "ffr_shocked_output", "lp__"
)
# Construct the state transition matrix A
# Adjust the dimensions and values according to your model
A <- matrix(c(
  params$Beta[1], params$tao[1], params$alpha[1], params$psi[1], params$gamma_pi[1], params$gamma_w[1],
  params$lambda_w[1], params$zeta_ps[1], params$zeta_ws[1], params$sigma_L[1], params$sigma_c[1], params$h[1],
  params$phi[1], params$phi_uppercase[1], params$rbar_k[1], params$Ky[1], params$sigma_eta_C[1], params$Invy[1],
  params$Cy[1], params$gy[1], params$r_pi[1], params$r_y[1], params$Ry_delta[1], params$rho[1],
  params$rho_el[1], params$rho_ea[1], params$rho_eb[1], params$rho_G[1], params$rho_pi[1], params$rh_i[1],
  params$rho_lamdaw[1], params$rho_q[1], params$rho_lamda_p[1], params$rho_L[1], params$output_growth_shocked[1], params$consumption_growth_shocked[1]
), nrow = 6, byrow = TRUE)  # Adjust the size based on your model's needs

# Ensure A is square by matching the number of rows and columns
if (nrow(A) != ncol(A)) {
  stop("The state transition matrix A is not square.")
}

# Calculate eigenvalues
eig_vals <- eigen(A)$values
 
# Calculate eigenvalues
eig_vals <- eigen(A)$values

# Identify and map eigenvalues with magnitudes greater than 1 to their variable names
unstable_eigenvalues <- eig_vals[abs(eig_vals) > 1]
unstable_variables <- variable_names[abs(eig_vals) > 1]

# Print unstable eigenvalues and corresponding variables
print("Unstable eigenvalues and corresponding variables (magnitude > 1):")
print(data.frame(variable = unstable_variables, eigenvalue = unstable_eigenvalues))

# Output all eigenvalues for inspection
print("All eigenvalues:")
print(eig_vals)
```


```{r}
library(bayesplot)
```
```{r}
setwd("C:/Users/gealy/OneDrive/Documents/New folder (2)")
getwd()

```
```{r}
# Assuming 'fit' is your Stan model object
summary_stats <- summary(fit)$summary

# Convert the matrix to a data frame where rows are parameters and columns are summary statistics
df_summary_stats <- as.data.frame(summary_stats)

# Set the row names as a column if needed
df_summary_stats$parameter <- rownames(df_summary_stats)

# Now df_summary_stats is a data frame with each parameter as a row and statistics as columns
print(df_summary_stats)

```
```{r}
# Assuming the parameters from your model are in the samples object
beta <- mean(samples$beta)
tao <- mean(samples$tao)
alpha <- mean(samples$alpha)
psi <- mean(samples$psi)
gamma_pi <- mean(samples$gamma_pi)
gamma_w <- mean(samples$gamma_w)
lambda_w <- mean(samples$lambda_w)
zeta_ps <- mean(samples$zeta_ps)
zeta_ws <- mean(samples$zeta_ws)
sigma_L <- mean(samples$sigma_L)
sigma_c <- mean(samples$sigma_c)
h <- mean(samples$h)
phi <- mean(samples$phi)
phi_uppercase <- mean(samples$phi_uppercase)
rbar_k <- mean(samples$rbar_k)
Ky <- mean(samples$Ky)
sigma_eta_C <- mean(samples$sigma_eta_C)
Invy <- mean(samples$Invy)
Cy <- mean(samples$Cy)
gy <- mean(samples$gy)
r_pi <- mean(samples$r_pi)
r_y <- mean(samples$r_y)
r_delta <- mean(samples$r_delta)
rho <- mean(samples$rho)
rho_el <- mean(samples$rho_el)
rho_ea <- mean(samples$rho_ea)
rho_eb <- mean(samples$rho_eb)
rho_G <- mean(samples$rho_G)
rho_pi <- mean(samples$rho_pi)
rh_i <- mean(samples$rh_i)
rho_lamdaw <- mean(samples$rho_lamdaw)
rho_q <- mean(samples$rho_q)
rho_lamda_p <- mean(samples$rho_lamda_p)
rho_L <- mean(samples$rho_L)
```


```{r}
simulate_shock_and_update <- function(data, shock_time, shock_value, beta, gamma_pi, gamma_w, h, phi, sigma_L, rho_r, rho_pi, rho_y, r_y, r_pi, r_delta) {
  N <- data$T
  output_growth_shock <- data$output_growth_vector
  consumption_growth_shock <- data$consumption_growth_vector
  investment_growth_shock <- data$investment_growth_vector
  wage_growth_shock <- data$wage_growth_vector
  hours_shock <- data$hours_vector
  inflation_shock <- data$inflation_vector
  ffr_shock <- data$ffr_vector

  # Apply the shock to output growth
  output_growth_shock[shock_time] <- output_growth_shock[shock_time] * (1 - shock_value / 100)

  # Propagate the shock through the model
  for (t in (shock_time + 1):N) {
    # Example model equations to update other series based on the shocked output growth
    consumption_growth_shock[t] <- h * consumption_growth_shock[t-1] + beta * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
    investment_growth_shock[t] <- phi * investment_growth_shock[t-1] + beta * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
    wage_growth_shock[t] <- gamma_w * wage_growth_shock[t-1] + beta * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
    hours_shock[t] <- gamma_pi * hours_shock[t-1] + beta * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
    inflation_shock[t] <- gamma_pi * inflation_shock[t-1] + rho_pi * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
    ffr_shock[t] <- gamma_pi * ffr_shock[t-1] + rho_r * output_growth_shock[t-1] + rnorm(1, 0, sigma_L)
  }

  return(list(
    output_growth_shock = output_growth_shock,
    consumption_growth_shock = consumption_growth_shock,
    investment_growth_shock = investment_growth_shock,
    wage_growth_shock = wage_growth_shock,
    hours_shock = hours_shock,
    inflation_shock = inflation_shock,
    ffr_shock = ffr_shock
  ))
}

# Apply a 30% decrease shock at a specific time point (e.g., t = 50)
shock_time <- 50  # This is the time point at which the shock is applied
shock_value <- 30  # 30% decrease

# Simulate the shocked output growth and update other series
shocked_series <- simulate_shock_and_update(data_list, shock_time, shock_value, beta, gamma_pi, gamma_w, h, phi, sigma_L, rho_r, rho_pi, rho_y, r_y, r_pi, r_delta)

# Filter for finite values in all series
output_growth_finite <- data_list$output_growth_vector[is.finite(data_list$output_growth_vector)]
output_growth_shock_finite <- shocked_series$output_growth_shock[is.finite(shocked_series$output_growth_shock)]

consumption_growth_finite <- data_list$consumption_growth_vector[is.finite(data_list$consumption_growth_vector)]
consumption_growth_shock_finite <- shocked_series$consumption_growth_shock[is.finite(shocked_series$consumption_growth_shock)]

investment_growth_finite <- data_list$investment_growth_vector[is.finite(data_list$investment_growth_vector)]
investment_growth_shock_finite <- shocked_series$investment_growth_shock[is.finite(shocked_series$investment_growth_shock)]

wage_growth_finite <- data_list$wage_growth_vector[is.finite(data_list$wage_growth_vector)]
wage_growth_shock_finite <- shocked_series$wage_growth_shock[is.finite(shocked_series$wage_growth_shock)]

hours_finite <- data_list$hours_vector[is.finite(data_list$hours_vector)]
hours_shock_finite <- shocked_series$hours_shock[is.finite(shocked_series$hours_shock)]

inflation_finite <- data_list$inflation_vector[is.finite(data_list$inflation_vector)]
inflation_shock_finite <- shocked_series$inflation_shock[is.finite(shocked_series$inflation_shock)]

ffr_finite <- data_list$ffr_vector[is.finite(data_list$ffr_vector)]
ffr_shock_finite <- shocked_series$ffr_shock[is.finite(shocked_series$ffr_shock)]

# Plot with finite values
par(mfrow = c(2, 3))  # Arrange plots in a 2x3 grid

plot(consumption_growth_finite, type = "l", col = "blue", lty = 2, ylim = range(c(consumption_growth_finite, consumption_growth_shock_finite), na.rm = TRUE), ylab = "Consumption Growth", xlab = "Time")
lines(consumption_growth_shock_finite, col = "red")

plot(investment_growth_finite, type = "l", col = "blue", lty = 2, ylim = range(c(investment_growth_finite, investment_growth_shock_finite), na.rm = TRUE), ylab = "Investment Growth", xlab = "Time")
lines(investment_growth_shock_finite, col = "red")

plot(wage_growth_finite, type = "l", col = "blue", lty = 2, ylim = range(c(wage_growth_finite, wage_growth_shock_finite), na.rm = TRUE), ylab = "Wage Growth", xlab = "Time")
lines(wage_growth_shock_finite, col = "red")

plot(hours_finite, type = "l", col = "blue", lty = 2, ylim = range(c(hours_finite, hours_shock_finite), na.rm = TRUE), ylab = "Hours", xlab = "Time")
lines(hours_shock_finite, col = "red")

plot(inflation_finite, type = "l", col = "blue", lty = 2, ylim = range(c(inflation_finite, inflation_shock_finite), na.rm = TRUE), ylab = "Inflation", xlab = "Time")
lines(inflation_shock_finite, col = "red")

plot(ffr_finite, type = "l", col = "blue", lty = 2, ylim = range(c(ffr_finite, ffr_shock_finite), na.rm = TRUE), ylab = "Federal Funds Rate", xlab = "Time")
lines(ffr_shock_finite, col = "red")

```


```{r}

options(scipen = 999)
# Ensure ggplot2 is loaded
library(ggplot2)
setwd("C:/Users/gealy/OneDrive/Documents/New folder (2)")
getwd()
# Ensure params are extracted with structure maintained
params <- extract(fit, permuted=FALSE)
 param_names <- dimnames(params)[[3]]  # Extract parameter names
color_scheme_set("brightblue")

# Loop through each parameter and generate plots
for (param_name in param_names) {
    if (param_name != "lp__") {  # Skip the log probability parameter
        param_data <- as.array(params[,, param_name])
        if (is.array(param_data)) {
            # Create a data frame for ggplot
            param_df <- data.frame(
                Iteration = rep(1:nrow(param_data), ncol(param_data)),
                Value = as.vector(t(param_data)),
                Chain = rep(1:ncol(param_data), each = nrow(param_data)),
                Parameter = param_name  # Include parameter name for clarity
            )
            
            # Generate the plot
            p <- ggplot(param_df, aes(x = Iteration, y = Value, color = as.factor(Chain))) +
                geom_line() +
                labs(title = paste("Trace Plot for", param_name), x = "Iteration", y = param_name) +
                theme_minimal()
            
            # Print the plot to the console or save to file
            print(p)
            # Optionally, save the plot to a file
            ggsave(paste0("Trace_", param_name, ".png"), plot = p, width = 10, height = 8)
            
            # Clear the plot if running in a script to save memory
            dev.off()  # This resets the plotting device
        } else {
            cat(param_name, "data could not be converted to an array correctly.\n")
        }
    }
}


```


```{r}
# Access data directly from the environment
beta_data <- get("Beta", envir = .GlobalEnv)

# Access data from the list
beta_data_from_list <- param_data_list[["Beta"]]

```

   

```{r}

library(actuar)
library(MASS)

```
```{r}
data_list <- list(
  output_growth_vector = final$Output_growth,
  consumption_growth_vector = final$Consumption_growth,
  investment_growth_vector = final$Investment_growth,
  wage_growth_vector = final$Wage_growth,
  hours_vector = final$Hours,
  inflation_vector = final$Inflation,
  ffr_vector = final$Ffr,
  kapital = my_data$K_hat,
  q = my_data$Q_hat
)

```


```{r}
library(MASS)  # Ensure MASS is loaded for fitdistr
```


```{r}
results <- list()  # Initialize results storage

# Loop through each data vector in the list
for (data_name in names(data_list)) {
    cat("Optimizing for", data_name, "\n")
    
    # Extract the current data vector
    current_data <- data_list[[data_name]]
    
    # Check if the current data vector is non-empty and numeric
    if (length(current_data) > 0 && is.numeric(current_data)) {
        # Fit the normal distribution to the current data vector
        fit_normal <- fitdistr(current_data, densfun = "normal")

        # Store the results
        results[[data_name]] <- fit_normal
        
        # Print the estimated parameters
        cat(data_name, "Estimated mean:", fit_normal$estimate["mean"], 
            "Estimated sd:", fit_normal$estimate["sd"], "\n")
    } else {
        cat(data_name, "data is not a non-empty numeric vector.\n")
    }
}

```























